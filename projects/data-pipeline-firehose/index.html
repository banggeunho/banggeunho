<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>사용자 이벤트 로그 데이터 파이프라인 | Geunho Bang</title>
  <link rel="stylesheet" href="../../style.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">
</head>
<body>
  <!-- Navigation -->
<nav class="nav scrolled" id="nav">
  <div class="nav-inner">
    <a href="../../index.html" class="nav-logo"><span class="logo-name">bang</span><span class="logo-dot">.geunho</span></a>
    <ul class="nav-links">
      <li><a href="../../index.html#projects" class="active">Projects</a></li>
      <li><a href="../../index.html#skills">Skills</a></li>
    </ul>
    <button class="nav-toggle" id="nav-toggle" aria-label="Toggle menu">
      <span></span><span></span><span></span>
    </button>
  </div>
</nav>


  <main class="project-detail">
    <div class="container">

      <!-- Back -->
      <a href="../../index.html#projects" class="back-link">
        <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M19 12H5m0 0 7 7m-7-7 7-7"/></svg>
        Projects
      </a>

      <!-- Hero image -->
      
      <div class="project-detail-hero">
        <img src="images/thumbnail.svg" alt="사용자 이벤트 로그 데이터 파이프라인">
      </div>
      

      <!-- Header -->
      <div class="project-detail-header">
        <h1>사용자 이벤트 로그 데이터 파이프라인</h1>
        
        <div class="project-detail-tags">
          
            <span>AWS Firehose</span>
          
            <span>S3</span>
          
            <span>Athena</span>
          
            <span>Redshift</span>
          
            <span>Tableau</span>
          
        </div>
        
      </div>

      <!-- Content -->
      <div class="project-detail-content">
        <section class="content-section">
<h2 id="목차">목차</h2>
<ol>
<li><a href="#%EB%B0%B0%EA%B2%BD-1-2%EC%9D%BC-%EA%B1%B8%EB%A6%AC%EB%8A%94-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EC%84%9D">배경: 1~2일 걸리는 데이터 분석</a></li>
<li><a href="#%EB%AC%B8%EC%A0%9C-%EB%B6%84%EC%84%9D-%EC%88%98%EB%8F%99-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%88%98%EC%A7%91%EC%9D%98-%ED%95%9C%EA%B3%84">문제 분석: 수동 데이터 수집의 한계</a></li>
<li><a href="#%ED%95%B4%EA%B2%B0-%EB%AA%A9%ED%91%9C-30%EB%B6%84-1%EC%8B%9C%EA%B0%84-%EC%9D%B4%EB%82%B4-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%9C%EA%B3%B5">해결 목표: 30분~1시간 이내 데이터 제공</a></li>
<li><a href="#%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98-%EC%84%A4%EA%B3%84-aws-%EA%B4%80%EB%A6%AC%ED%98%95-%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%84%A0%ED%83%9D-%EC%9D%B4%EC%9C%A0">아키텍처 설계: AWS 관리형 서비스 선택 이유</a></li>
<li><a href="#%ED%95%B5%EC%8B%AC-%EA%B5%AC%ED%98%84-1-firehose%EB%A1%9C-%EC%8B%A4%EC%8B%9C%EA%B0%84-%EC%88%98%EC%A7%91">핵심 구현 1: Firehose로 실시간 수집</a></li>
<li><a href="#%ED%95%B5%EC%8B%AC-%EA%B5%AC%ED%98%84-2-parquet-%EB%B3%80%ED%99%98%EC%9C%BC%EB%A1%9C-%EB%B9%84%EC%9A%A9-70-%EC%A0%88%EA%B0%90">핵심 구현 2: Parquet 변환으로 비용 70% 절감</a></li>
<li><a href="#%ED%95%B5%EC%8B%AC-%EA%B5%AC%ED%98%84-3-athena%EC%99%80-redshift%EB%A1%9C-%EB%B6%84%EC%84%9D-%EC%9E%90%EB%8F%99%ED%99%94">핵심 구현 3: Athena와 Redshift로 분석 자동화</a></li>
<li><a href="#%EA%B2%B0%EA%B3%BC-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%A6%AC%EB%93%9C%ED%83%80%EC%9E%84-1-2%EC%9D%BC-30%EB%B6%84">결과: 데이터 리드타임 1~2일→30분</a></li>
</ol>
<hr>
</section>
<section class="content-section">
<h2 id="배경-12일-걸리는-데이터-분석">배경: 1~2일 걸리는 데이터 분석</h2>
<p>&quot;지난 주 프로모션 배너의 클릭률을 분석하고 싶어요.&quot;</p>
<p>마케팅팀의 요청입니다. 하지만 데이터를 전달받기까지 평균 1~2일이 걸렸습니다. 그 사이 캠페인은 이미 끝나있고, 다음 기획은 과거 데이터 없이 진행될 수밖에 없었습니다.</p>
<p><strong>기존 프로세스:</strong></p>
<pre><code class="language-">1. 데이터 담당자가 어드민에서 직접 다운로드 또는 개발팀에 데이터 요청
2. 개발팀이 DB 직접 조회 후 전달
3. 데이터 담당자가 전처리 (중복 제거, 포맷 변환)
4. 데이터 웨어하우스에 수동 적재
5. 1~2일 후 분석 가능</code></pre><p>개발자는 반복적인 데이터 추출 작업에 시간을 빼앗기고, 데이터 담당자는 적시에 데이터를 확보하지 못하는 상황이었습니다.</p>
<hr>
</section>
<section class="content-section">
<h2 id="문제-분석-수동-데이터-수집의-한계">문제 분석: 수동 데이터 수집의 한계</h2>
<h3 id="수동-프로세스의-문제점">수동 프로세스의 문제점</h3>
<p><strong>1. 데이터 리드타임 (1~2일)</strong></p>
<pre><code class="language-">- 요청 접수: 수 시간 (업무 시간대 차이)
- DB 조회: 2시간 (쿼리 작성 + 실행)
- 데이터 전처리: 4시간 (중복 제거, 포맷 변환)
- 적재: 1시간
- 합계: 최소 반나절 → 보통 1~2일</code></pre><p><strong>2. 개발자 리소스 낭비</strong></p>
<ul>
<li>주당 5~10건의 데이터 추출 요청</li>
<li>개발자 1명이 주 10시간 소비</li>
<li>월 40시간 = 1주일치 생산성 손실</li>
</ul>
<p><strong>3. 데이터 정합성 이슈</strong></p>
<pre><code class="language-python"># 수동 전처리 과정에서 실수 가능
df = df.drop_duplicates()  # 어떤 컬럼 기준?
df['date'] = pd.to_datetime(df['date'])  # 포맷 일관성?
df.to_csv('output.csv', encoding='utf-8')  # 인코딩 이슈?</code></pre><p><strong>4. 히스토리 추적 불가</strong></p>
<ul>
<li>어제의 데이터와 오늘의 데이터가 다르면 원인 파악이 어렵습니다.</li>
<li>누가, 언제, 어떤 로직으로 전처리했는지 기록이 남지 않습니다.</li>
</ul>
<hr>
</section>
<section class="content-section">
<h2 id="해결-목표-30분1시간-이내-데이터-제공">해결 목표: 30분~1시간 이내 데이터 제공</h2>
<h3 id="정량적-목표">정량적 목표</h3>
<ul>
<li><strong>데이터 리드타임</strong>: 1<del>2일 → 30분</del>1시간 이내</li>
<li><strong>처리 용량</strong>: 월 평균 3,000만 건, 피크 5,000만 건</li>
<li><strong>스토리지 비용</strong>: 70% 절감 (Parquet 적용)</li>
<li><strong>Athena 쿼리 속도</strong>: 5배 향상</li>
</ul>
<h3 id="정성적-목표">정성적 목표</h3>
<ul>
<li>개발자 개입 없이 자동화</li>
<li>Tableau 대시보드 자동 업데이트</li>
<li>데이터 유실률 0%</li>
<li>히스토리 추적 가능 (S3 버전 관리)</li>
</ul>
<hr>
</section>
<section class="content-section">
<h2 id="아키텍처-설계-aws-관리형-서비스-선택-이유">아키텍처 설계: AWS 관리형 서비스 선택 이유</h2>
<h3 id="직접-구축-vs-aws-관리형">직접 구축 vs AWS 관리형</h3>
<table>
<thead>
<tr>
<th>기준</th>
<th>Kafka + Spark</th>
<th>AWS Firehose</th>
</tr>
</thead>
<tbody><tr>
<td>초기 구축 시간</td>
<td>2주</td>
<td>1일</td>
</tr>
<tr>
<td>운영 부담</td>
<td>높음 (클러스터 관리)</td>
<td>없음 (완전 관리형)</td>
</tr>
<tr>
<td>비용</td>
<td>높음 (EC2 상시 운영)</td>
<td>종량제</td>
</tr>
<tr>
<td>확장성</td>
<td>수동 스케일링</td>
<td>자동 무한 확장</td>
</tr>
<tr>
<td>학습 곡선</td>
<td>높음</td>
<td>낮음</td>
</tr>
</tbody></table>
<p><strong>AWS 관리형 선택 이유:</strong></p>
<ul>
<li>트래픽이 적을 때는 비용 거의 0원</li>
<li>서버 관리 불필요</li>
<li>5,000만 건/월 피크에도 문제없이 처리</li>
</ul>
<h3 id="전체-아키텍처">전체 아키텍처</h3>
<pre class="mermaid">graph TB
    subgraph 이벤트_수집["① 이벤트 수집"]
        A[사용자 행동<br/>구매 · 조회 · 검색 · 클릭] -->|이벤트 전송| B[NestJS API<br/>putRecordBatch]
    end

    subgraph 수집_파이프라인["② 수집 파이프라인 (AWS Managed)"]
        B -->|JSON 스트림| C[AWS Firehose<br/>버퍼링: 5MB / 5분]
        C -->|배치 전달| D[Lambda<br/>Parquet + Snappy 변환]
    end

    subgraph 저장["③ 저장"]
        D -->|Parquet 파일| E["S3<br/>year=/month=/day= 파티셔닝"]
    end

    subgraph 분석_레이어["④ 분석 레이어"]
        E -->|"즉시 조회 (5분 지연)"| F[Athena<br/>서버리스 쿼리]
        E -->|"매일 새벽 COPY"| G[Redshift<br/>데이터 웨어하우스]
    end

    subgraph 시각화["⑤ 시각화"]
        F -->|실시간 대시보드| H[Tableau]
        G -->|정기 리포트| H
    end</pre><h3 id="수집-이벤트-8개">수집 이벤트 (8개)</h3>
<pre><code class="language-typescript">// 구매
POST /events/purchase
{ userId, productId, amount, timestamp }

// 상품상세 조회
POST /events/view
{ userId, productId, timestamp }

// 장바구니 담기
POST /events/cart
{ userId, productId, timestamp }

// 검색
POST /events/search
{ userId, keyword, timestamp }

// 배너 클릭/노출
POST /events/banner
{ userId, bannerId, type: 'click' | 'view', timestamp }

// 페이지 이동
POST /events/pageview
{ userId, page, referrer, timestamp }

// 회원가입
POST /events/signup
{ userId, channel, timestamp }

// 로그인
POST /events/login
{ userId, timestamp }</code></pre><hr>
</section>
<section class="content-section">
<h2 id="핵심-구현-1-firehose로-실시간-수집">핵심 구현 1: Firehose로 실시간 수집</h2>
<h3 id="firehose-설정">Firehose 설정</h3>
<p><strong>CloudFormation 템플릿:</strong></p>
<pre><code class="language-yaml">EventsFirehose:
  Type: AWS::KinesisFirehose::DeliveryStream
  Properties:
    DeliveryStreamName: user-events-stream
    ExtendedS3DestinationConfiguration:
      BucketARN: !GetAtt EventsBucket.Arn
      Prefix: events/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/
      ErrorOutputPrefix: errors/
      CompressionFormat: GZIP
      BufferingHints:
        SizeInMBs: 5
        IntervalInSeconds: 300  # 5분마다 저장
      ProcessingConfiguration:
        Enabled: true
        Processors:
          - Type: Lambda
            Parameters:
              - ParameterName: LambdaArn
                ParameterValue: !GetAtt ParquetTransformLambda.Arn</code></pre><p><strong>버퍼링 설정 선택 이유:</strong></p>
<ul>
<li>5MB 또는 5분 중 먼저 도달하는 조건으로 동작합니다.</li>
<li>트래픽이 적으면 5분 대기 (비용 절감)</li>
<li>트래픽이 많으면 5MB마다 저장 (지연 최소화)</li>
</ul>
<h3 id="api-통합">API 통합</h3>
<p><strong>NestJS 이벤트 전송:</strong></p>
<pre><code class="language-typescript">@Injectable()
export class EventService {
  constructor(
    private readonly firehose: AWS.Firehose
  ) {}

  async trackEvent(event: UserEvent) {
    // Firehose에 전송
    await this.firehose.putRecord({
      DeliveryStreamName: 'user-events-stream',
      Record: {
        Data: JSON.stringify({
          ...event,
          timestamp: Date.now(),
          server_timestamp: new Date().toISOString()
        }) + '\n'  // 줄바꿈 필수 (Athena 파싱)
      }
    }).promise();
  }
}</code></pre><p><strong>배치 전송 최적화:</strong></p>
<pre><code class="language-typescript">// 개별 전송 (느림)
for (const event of events) {
  await firehose.putRecord({ ... });  // 10ms × 100건 = 1초
}

// 배치 전송 (빠름)
await firehose.putRecordBatch({
  DeliveryStreamName: 'user-events-stream',
  Records: events.map(e => ({
    Data: JSON.stringify(e) + '\n'
  }))
});  // 100ms × 1번 = 100ms</code></pre><hr>
</section>
<section class="content-section">
<h2 id="핵심-구현-2-parquet-변환으로-비용-70-절감">핵심 구현 2: Parquet 변환으로 비용 70% 절감</h2>
<h3 id="json-vs-parquet">JSON vs Parquet</h3>
<table>
<thead>
<tr>
<th>포맷</th>
<th>크기 (100만 건)</th>
<th>Athena 스캔 비용</th>
<th>쿼리 속도</th>
</tr>
</thead>
<tbody><tr>
<td>JSON (GZIP)</td>
<td>1GB</td>
<td>$5/TB × 1GB = $0.005</td>
<td>10초</td>
</tr>
<tr>
<td>Parquet</td>
<td>300MB</td>
<td>$5/TB × 0.3GB = $0.0015</td>
<td>2초</td>
</tr>
<tr>
<td><strong>절감</strong></td>
<td><strong>70%</strong></td>
<td><strong>70%</strong></td>
<td><strong>5배</strong></td>
</tr>
</tbody></table>
<h3 id="lambda-transform-함수">Lambda Transform 함수</h3>
<p><strong>Parquet 변환 (PyArrow):</strong></p>
<p>Firehose는 이벤트를 배치(최대 6MB 또는 900초)로 묶어서 Lambda에 전달합니다. Lambda는 배치 내 모든 레코드를 변환하여 반환합니다.</p>
<pre><code class="language-python">import json
import base64
import pyarrow as pa
import pyarrow.parquet as pq
from io import BytesIO

def lambda_handler(event, context):
    output = []

    # Firehose가 배치로 전달한 레코드들을 순회
    for record in event['records']:
        # 1. Base64 디코딩
        payload = base64.b64decode(record['data']).decode('utf-8')
        data = json.loads(payload)

        # 2. 스키마 정의
        schema = pa.schema([
            ('user_id', pa.int64()),
            ('event_type', pa.string()),
            ('product_id', pa.int64()),
            ('amount', pa.float64()),
            ('timestamp', pa.timestamp('ms')),
            ('server_timestamp', pa.timestamp('ms'))
        ])

        # 3. Parquet로 변환
        table = pa.Table.from_pydict({
            'user_id': [data['userId']],
            'event_type': [data['eventType']],
            'product_id': [data.get('productId')],
            'amount': [data.get('amount')],
            'timestamp': [pa.scalar(data['timestamp'], type=pa.timestamp('ms'))],
            'server_timestamp': [pa.scalar(data['server_timestamp'], type=pa.timestamp('ms'))]
        }, schema=schema)

        # 4. 바이너리로 변환
        buf = BytesIO()
        pq.write_table(table, buf, compression='snappy')

        output.append({
            'recordId': record['recordId'],
            'result': 'Ok',
            'data': base64.b64encode(buf.getvalue()).decode('utf-8')
        })

    return {'records': output}</code></pre><p><strong>압축 알고리즘 비교:</strong></p>
<pre><code class="language-">- None: 500MB (빠름, 비쌈)
- GZIP: 300MB (느림, 저렴)
- Snappy: 350MB (빠름, 중간) ✅ 선택</code></pre><hr>
</section>
<section class="content-section">
<h2 id="핵심-구현-3-athena와-redshift로-분석-자동화">핵심 구현 3: Athena와 Redshift로 분석 자동화</h2>
<h3 id="athena-테이블-생성">Athena 테이블 생성</h3>
<p><strong>Glue Crawler 자동 스키마 감지:</strong></p>
<pre><code class="language-yaml">EventsCrawler:
  Type: AWS::Glue::Crawler
  Properties:
    Name: events-crawler
    Role: !GetAtt GlueServiceRole.Arn
    DatabaseName: analytics
    Targets:
      S3Targets:
        - Path: s3://my-events-bucket/events/
    Schedule:
      ScheduleExpression: cron(0 */6 * * ? *)  # 6시간마다 실행</code></pre><p><strong>Athena 쿼리 (즉시 조회):</strong></p>
<pre><code class="language-sql">-- 일별 구매 금액
SELECT
  DATE(from_unixtime(timestamp / 1000)) as date,
  COUNT(*) as purchase_count,
  SUM(amount) as total_amount
FROM events
WHERE event_type = 'purchase'
  AND year = '2024' AND month = '05'
GROUP BY DATE(from_unixtime(timestamp / 1000))
ORDER BY date DESC;</code></pre><p><strong>실행 시간:</strong></p>
<pre><code class="language-">Before (JSON):
- 스캔 데이터: 10GB
- 실행 시간: 50초
- 비용: $0.05

After (Parquet):
- 스캔 데이터: 2GB
- 실행 시간: 10초 (5배 향상)
- 비용: $0.01 (80% 절감)</code></pre><h3 id="redshift-적재-배치">Redshift 적재 (배치)</h3>
<p><strong>매일 새벽 2시 배치:</strong></p>
<pre><code class="language-sql">-- COPY 명령어로 S3 → Redshift
COPY analytics.events
FROM 's3://my-events-bucket/events/year=2024/month=05/day=08/'
IAM_ROLE 'arn:aws:iam::123456789:role/RedshiftS3Role'
FORMAT AS PARQUET;</code></pre><p><strong>테이블 구조:</strong></p>
<pre><code class="language-sql">CREATE TABLE analytics.events (
  user_id BIGINT,
  event_type VARCHAR(50),
  product_id BIGINT,
  amount DECIMAL(10,2),
  timestamp TIMESTAMP,
  server_timestamp TIMESTAMP
)
DISTKEY(user_id)
SORTKEY(timestamp);</code></pre><p>S3의 Hive 스타일 파티셔닝(<code>year=/month=/day=</code>)은 Athena에서 자동으로 파티션으로 인식되며, Redshift에는 날짜별로 COPY 명령을 실행하여 적재합니다.</p>
<h3 id="tableau-연동">Tableau 연동</h3>
<p><strong>Athena 커넥터:</strong></p>
<ul>
<li>실시간 대시보드 (30분~1시간 지연)</li>
<li>빠른 프로토타이핑</li>
</ul>
<p><strong>Redshift 커넥터:</strong></p>
<ul>
<li>정기 리포트 (1일 지연)</li>
<li>복잡한 조인 쿼리</li>
</ul>
<hr>
</section>
<section class="content-section">
<h2 id="결과-데이터-리드타임-12일30분">결과: 데이터 리드타임 1~2일→30분</h2>
<h3 id="리드타임-단축">리드타임 단축</h3>
<table>
<thead>
<tr>
<th>단계</th>
<th>Before</th>
<th>After</th>
</tr>
</thead>
<tbody><tr>
<td>데이터 수집</td>
<td>어드민 다운로드 / 개발팀 요청</td>
<td>자동 (Firehose)</td>
</tr>
<tr>
<td>전처리</td>
<td>수동 (4시간)</td>
<td>자동 (Lambda)</td>
</tr>
<tr>
<td>적재</td>
<td>수동 (1시간)</td>
<td>자동 (S3)</td>
</tr>
<tr>
<td><strong>리드타임</strong></td>
<td><strong>1~2일</strong></td>
<td><strong>30분~1시간</strong></td>
</tr>
</tbody></table>
<p>기존에는 데이터 담당자가 어드민에서 직접 다운로드하거나 개발팀에 데이터를 요청한 뒤, 전처리를 거쳐 수동으로 적재해야 했습니다. 파이프라인 구축 후 이 과정이 완전히 자동화되었습니다.</p>
<h3 id="비용">비용</h3>
<table>
<thead>
<tr>
<th>항목</th>
<th>내용</th>
</tr>
</thead>
<tbody><tr>
<td><strong>추가 인프라 비용</strong></td>
<td>월 약 10만원</td>
</tr>
<tr>
<td><strong>스토리지 절감</strong></td>
<td>Parquet 적용으로 70% 절감</td>
</tr>
<tr>
<td><strong>개발자 시간 절약</strong></td>
<td>월 40시간 (데이터 추출 작업 제거)</td>
</tr>
</tbody></table>
<h3 id="처리-성능">처리 성능</h3>
<pre><code class="language-">- 월 평균 처리량: 3,000만 건
- 월 피크 처리량: 5,000만 건
- 일 평균: 100만 건
- 데이터 유실: 측정 기간 6개월간 0건 (Firehose 자동 재시도)</code></pre><h3 id="자동화-성과">자동화 성과</h3>
<ul>
<li>수동 데이터 추출 요청: 0건</li>
<li>Tableau 대시보드 자동 업데이트: 8개</li>
<li>개발팀 데이터 요청 대응 시간: 1~2일 → 불필요</li>
</ul>
<hr>
</section>
<section class="content-section">
<h2 id="배운-점">배운 점</h2>
<p><strong>1. AWS 관리형 서비스의 위력</strong></p>
<ul>
<li>Firehose + Lambda + S3로 완전 자동화를 달성했습니다.</li>
<li>서버 관리 불필요, 종량제 과금으로 비용을 최소화했습니다.</li>
</ul>
<p><strong>2. Parquet는 필수</strong></p>
<ul>
<li>스토리지 비용 70% 절감</li>
<li>Athena 쿼리 속도 5배 향상</li>
<li>압축은 Snappy (빠르고 적절한 압축률)</li>
</ul>
<p><strong>3. 분석 레이어 이원화</strong></p>
<ul>
<li>Athena: 실시간 조회 (30분~1시간 지연)</li>
<li>Redshift: 배치 적재 (1일 지연)</li>
<li>용도에 따라 적절한 도구를 선택하는 것이 중요합니다.</li>
</ul>
<p><strong>4. 데이터 파티셔닝</strong></p>
<pre><code class="language-">s3://bucket/events/year=2024/month=05/day=08/</code></pre><ul>
<li>날짜별 파티션으로 쿼리 성능을 향상시켰습니다.</li>
<li>불필요한 데이터 스캔을 방지하여 비용도 절감됩니다.</li>
</ul>
<p><strong>5. 히스토리 추적</strong></p>
<ul>
<li>S3 버전 관리로 데이터 복구가 가능합니다.</li>
<li>Glue Crawler로 스키마 변경을 자동으로 감지합니다.</li>
</ul>
<hr>
</section>
<section class="content-section">
<h2 id="기술-스택">기술 스택</h2>
<table>
<thead>
<tr>
<th>분류</th>
<th>기술</th>
</tr>
</thead>
<tbody><tr>
<td><strong>이벤트 수집</strong></td>
<td>AWS Kinesis Data Firehose</td>
</tr>
<tr>
<td><strong>데이터 변환</strong></td>
<td>AWS Lambda (Python, PyArrow)</td>
</tr>
<tr>
<td><strong>스토리지</strong></td>
<td>S3 (Parquet + Snappy)</td>
</tr>
<tr>
<td><strong>쿼리 엔진</strong></td>
<td>Athena (서버리스)</td>
</tr>
<tr>
<td><strong>데이터 웨어하우스</strong></td>
<td>Redshift</td>
</tr>
<tr>
<td><strong>스키마 관리</strong></td>
<td>AWS Glue Crawler</td>
</tr>
<tr>
<td><strong>시각화</strong></td>
<td>Tableau</td>
</tr>
</tbody></table>
</section>

      </div>

    </div>
  </main>

  <footer class="footer">
  <div class="container">
    <p>&copy; 2025 Geunho Bang. Built with passion.</p>
  </div>
</footer>


  <script src="../../script.js"></script>

  <!-- Mermaid for diagrams -->
  <script type="module">
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    mermaid.initialize({
      startOnLoad: true,
      theme: 'base',
      themeVariables: {
        primaryColor: '#0366d6',
        primaryTextColor: '#24292e',
        primaryBorderColor: '#e1e4e8',
        lineColor: '#586069',
        secondaryColor: '#f6f8fa',
        tertiaryColor: '#fafbfc',
        background: '#ffffff',
        mainBkg: '#ffffff',
        secondBkg: '#f6f8fa',
        border1: '#e1e4e8',
        border2: '#e1e4e8',
        note: '#fff5b1',
        noteBkg: '#fff5b1',
        noteBorder: '#e1e4e8',
        noteText: '#24292e',
        fontSize: '14px',
        fontFamily: 'Inter, -apple-system, sans-serif'
      }
    });
  </script>

  <!-- Prism for syntax highlighting -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-java.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-javascript.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-typescript.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-bash.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-sql.min.js"></script>
</body>
</html>
