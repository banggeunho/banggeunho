[
  {
    "id": "data-pipeline-firehose",
    "title": "사용자 이벤트 로그 데이터 파이프라인",
    "tags": [
      "AWS Firehose",
      "S3",
      "Athena",
      "Redshift",
      "Tableau"
    ],
    "content": "<section class=\"content-section\">\n<h2 id=\"목차\">목차</h2>\n<ol>\n<li><a href=\"#%EB%B0%B0%EA%B2%BD-12%EC%9D%BC-%EA%B1%B8%EB%A6%AC%EB%8A%94-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%B6%84%EC%84%9D\">배경: 1~2일 걸리는 데이터 분석</a></li>\n<li><a href=\"#%EB%AC%B8%EC%A0%9C-%EB%B6%84%EC%84%9D-%EC%88%98%EB%8F%99-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%88%98%EC%A7%91%EC%9D%98-%ED%95%9C%EA%B3%84\">문제 분석: 수동 데이터 수집의 한계</a></li>\n<li><a href=\"#%ED%95%B4%EA%B2%B0-%EB%AA%A9%ED%91%9C-%EB%8B%B9%EC%9D%BC-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%A0%9C%EA%B3%B5\">해결 목표: 당일 데이터 제공</a></li>\n<li><a href=\"#%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98-%EC%84%A4%EA%B3%84-aws-%EA%B4%80%EB%A6%AC%ED%98%95-%EC%84%9C%EB%B9%84%EC%8A%A4-%EC%84%A0%ED%83%9D-%EC%9D%B4%EC%9C%A0\">아키텍처 설계: AWS 관리형 서비스 선택 이유</a></li>\n<li><a href=\"#%ED%95%B5%EC%8B%AC-%EA%B5%AC%ED%98%84-1-firehose%EB%A1%9C-%EC%8B%A4%EC%8B%9C%EA%B0%84-%EC%88%98%EC%A7%91\">핵심 구현 1: Firehose로 실시간 수집</a></li>\n<li><a href=\"#%ED%95%B5%EC%8B%AC-%EA%B5%AC%ED%98%84-2-parquet-%EB%B3%80%ED%99%98%EC%9C%BC%EB%A1%9C-%EB%B9%84%EC%9A%A9-70-%EC%A0%88%EA%B0%90\">핵심 구현 2: Parquet 변환으로 비용 70% 절감</a></li>\n<li><a href=\"#%ED%95%B5%EC%8B%AC-%EA%B5%AC%ED%98%84-3-athena%EC%99%80-redshift%EB%A1%9C-%EB%B6%84%EC%84%9D-%EC%9E%90%EB%8F%99%ED%99%94\">핵심 구현 3: Athena와 Redshift로 분석 자동화</a></li>\n<li><a href=\"#%EA%B2%B0%EA%B3%BC-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%A6%AC%EB%93%9C%ED%83%80%EC%9E%84-12%EC%9D%BC30%EB%B6%84\">결과: 데이터 리드타임 1~2일→30분</a></li>\n</ol>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"배경-12일-걸리는-데이터-분석\">배경: 1~2일 걸리는 데이터 분석</h2>\n<p>&quot;지난 주 프로모션 배너의 클릭률을 분석하고 싶어요.&quot;</p>\n<p>마케팅팀의 요청입니다. 하지만 데이터를 전달받기까지 평균 1~2일이 걸렸습니다. 그 사이 캠페인은 이미 끝나있고, 다음 기획은 과거 데이터 없이 진행될 수밖에 없었습니다.</p>\n<p><strong>기존 프로세스:</strong></p>\n<pre><code class=\"language-\">1. 데이터 담당자가 어드민에서 직접 다운로드 또는 개발팀에 데이터 요청\n2. 개발팀이 DB 직접 조회 후 전달\n3. 데이터 담당자가 전처리 (중복 제거, 포맷 변환)\n4. 데이터 웨어하우스에 수동 적재\n5. 1~2일 후 분석 가능</code></pre><p>개발자는 반복적인 데이터 추출 작업에 시간을 빼앗기고, 데이터 담당자는 적시에 데이터를 확보하지 못하는 상황이었습니다.</p>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"문제-분석-수동-데이터-수집의-한계\">문제 분석: 수동 데이터 수집의 한계</h2>\n<h3 id=\"수동-프로세스의-문제점\">수동 프로세스의 문제점</h3>\n<p><strong>1. 데이터 리드타임 (1~2일)</strong></p>\n<pre><code class=\"language-\">- 요청 접수: 수 시간 (업무 시간대 차이)\n- DB 조회: 1시간 (쿼리 작성 + 실행)\n- 데이터 전처리: 1시간 (중복 제거, 포맷 변환)\n- 적재: 1시간\n- 합계: 최소 반나절 → 보통 1~2일</code></pre><p><strong>2. 개발자 리소스 낭비</strong></p>\n<ul>\n<li>주당 5~10건의 데이터 추출 요청</li>\n<li>개발자 1명이 주 10시간 소비</li>\n<li>월 40시간 = 1주일치 생산성 손실</li>\n</ul>\n<p><strong>3. 데이터 정합성 이슈</strong></p>\n<pre><code class=\"language-python\"># 수동 전처리 과정에서 실수 가능\ndf = df.drop_duplicates()  # 어떤 컬럼 기준?\ndf['date'] = pd.to_datetime(df['date'])  # 포맷 일관성?\ndf.to_csv('output.csv', encoding='utf-8')  # 인코딩 이슈?</code></pre><p><strong>4. 히스토리 추적 불가</strong></p>\n<ul>\n<li>어제의 데이터와 오늘의 데이터가 다르면 원인 파악이 어렵습니다.</li>\n<li>누가, 언제, 어떤 로직으로 전처리했는지 기록이 남지 않습니다.</li>\n</ul>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"해결-목표-당일-데이터-제공\">해결 목표: 당일 데이터 제공</h2>\n<h3 id=\"정량적-목표\">정량적 목표</h3>\n<ul>\n<li><strong>데이터 리드타임</strong>: 1~2일 → 당일 데이터 리드 가능</li>\n<li><strong>처리 용량</strong>: 월 평균 3,000만 건, 피크 5,000만 건</li>\n</ul>\n<h3 id=\"정성적-목표\">정성적 목표</h3>\n<ul>\n<li>개발자 개입 없이 자동화</li>\n<li>Tableau 대시보드 자동 업데이트</li>\n<li>데이터 유실률 0%</li>\n</ul>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"아키텍처-설계-aws-관리형-서비스-선택-이유\">아키텍처 설계: AWS 관리형 서비스 선택 이유</h2>\n<h3 id=\"직접-구축-vs-aws-관리형\">직접 구축 vs AWS 관리형</h3>\n<table>\n<thead>\n<tr>\n<th>기준</th>\n<th>Kafka + Spark</th>\n<th>AWS Firehose</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>초기 구축 시간</td>\n<td>2주</td>\n<td>1일</td>\n</tr>\n<tr>\n<td>운영 부담</td>\n<td>높음 (클러스터 관리)</td>\n<td>없음 (완전 관리형)</td>\n</tr>\n<tr>\n<td>비용</td>\n<td>높음 (EC2 상시 운영)</td>\n<td>종량제</td>\n</tr>\n<tr>\n<td>확장성</td>\n<td>수동 스케일링</td>\n<td>자동 무한 확장</td>\n</tr>\n<tr>\n<td>학습 곡선</td>\n<td>높음</td>\n<td>낮음</td>\n</tr>\n</tbody></table>\n<p><strong>AWS 관리형 선택 이유:</strong></p>\n<ul>\n<li>트래픽이 적을 때는 비용 거의 0원</li>\n<li>서버 관리 불필요</li>\n<li>5,000만 건/월 피크에도 문제없이 처리</li>\n</ul>\n<h3 id=\"전체-아키텍처\">전체 아키텍처</h3>\n<pre class=\"mermaid\">graph TB\n    subgraph 이벤트_수집[\"① 이벤트 수집\"]\n        A[사용자 행동<br/>구매 · 조회 · 검색 · 클릭] -->|이벤트 전송| B[NestJS API<br/>putRecordBatch]\n    end\n\n    subgraph 수집_파이프라인[\"② 수집 파이프라인 (AWS Managed)\"]\n        B -->|JSON 스트림| C[AWS Firehose<br/>버퍼링: 64MB / 5분]\n        C -->|네이티브 변환<br/>Glue Schema 참조| D[Parquet + Snappy]\n    end\n\n    subgraph 저장[\"③ 저장\"]\n        D -->|Parquet 파일| E[\"S3<br/>year=/month=/day=/hour= 파티셔닝\"]\n    end\n\n    subgraph 분석_레이어[\"④ 분석 레이어\"]\n        E -->|\"즉시 조회 (5분 지연)\"| F[Athena<br/>서버리스 쿼리]\n        E -->|\"30분 증분 COPY\"| G[Redshift<br/>데이터 웨어하우스]\n    end\n\n    subgraph 시각화[\"⑤ 시각화\"]\n        F -->|실시간 대시보드| H[Tableau]\n        G -->|정기 리포트| H\n    end</pre><h3 id=\"수집-이벤트-예시\">수집 이벤트 예시</h3>\n<pre><code class=\"language-typescript\">// 구매\nPOST /events/purchase\n{ eventId, userId, productId, amount, timestamp }\n\n// 상품상세 조회\nPOST /events/view\n{ eventId, userId, productId, timestamp }\n\n// 장바구니 담기\nPOST /events/cart\n{ eventId, userId, productId, timestamp }\n\n// 검색\nPOST /events/search\n{ eventId, userId, keyword, timestamp }\n\n// 배너 클릭/노출\nPOST /events/banner\n{ eventId, userId, bannerId, type: 'click' | 'view', timestamp }\n\n// 페이지 이동\nPOST /events/pageview\n{ eventId, userId, page, referrer, timestamp }\n\n// 회원가입\nPOST /events/signup\n{ eventId, userId, channel, timestamp }\n\n// 로그인\nPOST /events/login\n{ eventId, userId, timestamp }</code></pre><hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"핵심-구현-1-firehose로-실시간-수집\">핵심 구현 1: Firehose로 실시간 수집</h2>\n<h3 id=\"firehose-설정\">Firehose 설정</h3>\n<p><strong>CloudFormation 템플릿:</strong></p>\n<pre><code class=\"language-yaml\">EventsFirehose:\n  Type: AWS::KinesisFirehose::DeliveryStream\n  Properties:\n    DeliveryStreamName: user-events-stream\n    ExtendedS3DestinationConfiguration:\n      BucketARN: !GetAtt EventsBucket.Arn\n      Prefix: events/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/\n      ErrorOutputPrefix: errors/\n      BufferingHints:\n        SizeInMBs: 64          # Parquet 네이티브 변환 시 최소 64MB 권장\n        IntervalInSeconds: 300  # 5분마다 저장\n      # Parquet 네이티브 변환은 &quot;핵심 구현 2&quot;에서 상세 설명\n      DataFormatConversionConfiguration:\n        Enabled: true</code></pre><p><strong>버퍼링 설정 선택 이유:</strong></p>\n<ul>\n<li>64MB 또는 5분 중 먼저 도달하는 조건으로 동작합니다.</li>\n<li>Parquet 네이티브 변환 시 AWS 권장 최소 버퍼 크기가 64MB</li>\n<li>버퍼가 클수록 하나의 Parquet 파일에 더 많은 레코드가 담겨 Athena 쿼리 효율 향상</li>\n<li>트래픽이 적으면 5분 대기 (비용 절감)</li>\n</ul>\n<h3 id=\"api-통합\">API 통합</h3>\n<p><strong>NestJS 이벤트 전송:</strong></p>\n<pre><code class=\"language-typescript\">@Injectable()\nexport class EventService {\n  constructor(\n    private readonly firehose: AWS.Firehose\n  ) {}\n\n  async trackEvent(event: UserEvent) {\n    // Firehose에 전송\n    await this.firehose.putRecord({\n      DeliveryStreamName: 'user-events-stream',\n      Record: {\n        Data: JSON.stringify({\n          event_id: event.eventId,   // dedup 기준 키\n          ...event,\n          timestamp: Date.now(),\n          server_timestamp: new Date().toISOString()\n        }) + '\\n'  // 줄바꿈 필수 (Athena 파싱)\n      }\n    }).promise();\n  }\n}</code></pre><p><strong>배치 전송 최적화:</strong></p>\n<pre><code class=\"language-typescript\">// 개별 전송 (느림)\nfor (const event of events) {\n  await firehose.putRecord({ ... });  // 10ms × 100건 = 1초\n}\n\n// 배치 전송 (빠름)\nawait firehose.putRecordBatch({\n  DeliveryStreamName: 'user-events-stream',\n  Records: events.map(e =&gt; ({\n    Data: JSON.stringify(e) + '\\n'\n  }))\n});  // 100ms × 1번 = 100ms</code></pre><hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"핵심-구현-2-parquet-변환으로-비용-70-절감\">핵심 구현 2: Parquet 변환으로 비용 70% 절감</h2>\n<h3 id=\"json-vs-parquet\">JSON vs Parquet</h3>\n<table>\n<thead>\n<tr>\n<th>포맷</th>\n<th>크기 (100만 건)</th>\n<th>Athena 스캔 비용</th>\n<th>쿼리 속도</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>JSON (GZIP)</td>\n<td>1GB</td>\n<td>$5/TB × 1GB = $0.005</td>\n<td>10초</td>\n</tr>\n<tr>\n<td>Parquet</td>\n<td>300MB</td>\n<td>$5/TB × 0.3GB = $0.0015</td>\n<td>2초</td>\n</tr>\n<tr>\n<td><strong>절감</strong></td>\n<td><strong>70%</strong></td>\n<td><strong>70%</strong></td>\n<td><strong>5배</strong></td>\n</tr>\n</tbody></table>\n<h3 id=\"firehose-네이티브-parquet-변환\">Firehose 네이티브 Parquet 변환</h3>\n<p>Parquet의 장점은 컬럼 스토리지로 <strong>대량의 레코드를 하나의 파일에 담았을 때</strong> 나타납니다. Lambda Transform에서 레코드를 개별 Parquet 파일로 변환하면 오히려 JSON보다 파일이 커지고, Athena에서 Small File Problem이 발생합니다.</p>\n<p>이를 해결하기 위해 <strong>Firehose의 네이티브 Record Format Conversion</strong> 기능을 활용했습니다. Firehose가 버퍼링한 배치 데이터를 Glue Data Catalog의 스키마 정의를 참조하여 Parquet으로 자동 변환합니다. Lambda Transform 없이도 Firehose 레벨에서 효율적인 Parquet 파일이 생성됩니다.</p>\n<p><strong>Glue Data Catalog 테이블 정의:</strong></p>\n<pre><code class=\"language-sql\">-- Glue Data Catalog에 스키마 등록\nCREATE EXTERNAL TABLE events_schema (\n  event_id STRING,\n  user_id BIGINT,\n  event_type STRING,\n  product_id BIGINT,\n  amount DOUBLE,\n  `timestamp` TIMESTAMP,\n  server_timestamp TIMESTAMP\n)\nSTORED AS PARQUET\nLOCATION 's3://my-events-bucket/events/'\nTBLPROPERTIES ('parquet.compression'='SNAPPY');</code></pre><p><strong>CloudFormation — Firehose Record Format Conversion 설정:</strong></p>\n<pre><code class=\"language-yaml\">EventsFirehose:\n  Type: AWS::KinesisFirehose::DeliveryStream\n  Properties:\n    DeliveryStreamName: user-events-stream\n    ExtendedS3DestinationConfiguration:\n      BucketARN: !GetAtt EventsBucket.Arn\n      Prefix: events/year=!{timestamp:yyyy}/month=!{timestamp:MM}/day=!{timestamp:dd}/hour=!{timestamp:HH}/\n      ErrorOutputPrefix: errors/\n      BufferingHints:\n        SizeInMBs: 64        # Parquet 변환 시 최소 64MB 권장\n        IntervalInSeconds: 300\n      # Parquet 네이티브 변환 설정\n      DataFormatConversionConfiguration:\n        Enabled: true\n        InputFormatConfiguration:\n          Deserializer:\n            OpenXJsonSerDe: {}    # JSON 입력 파싱\n        OutputFormatConfiguration:\n          Serializer:\n            ParquetSerDe:\n              Compression: SNAPPY  # 빠른 압축 + 적절한 압축률\n        SchemaConfiguration:\n          DatabaseName: !Ref GlueDatabase\n          TableName: !Ref GlueTable\n          RoleARN: !GetAtt FirehoseRole.Arn</code></pre><p><strong>네이티브 변환의 장점:</strong></p>\n<ul>\n<li>Firehose가 버퍼링한 데이터를 <strong>한 파일로 묶어</strong> Parquet 변환 → Small File Problem 방지</li>\n<li>Lambda Transform 비용 제거 (Lambda 실행 비용, 메모리 비용 불필요)</li>\n<li>Glue Data Catalog 기반 스키마 관리로 스키마 변경이 중앙화</li>\n</ul>\n<p><strong>압축 알고리즘 비교:</strong></p>\n<pre><code class=\"language-\">- None: 500MB (빠름, 비쌈)\n- GZIP: 300MB (느림, 저렴)\n- Snappy: 350MB (빠름, 중간) ✅ 선택</code></pre><h3 id=\"스키마-진화schema-evolution-전략\">스키마 진화(Schema Evolution) 전략</h3>\n<p>이벤트 타입이 추가되거나 필드가 변경되는 것은 데이터 파이프라인에서 가장 빈번하게 발생하는 변경 사항입니다.</p>\n<p><strong>하위 호환성 원칙:</strong></p>\n<ul>\n<li>새 필드는 <strong>nullable로만 추가</strong> (기존 레코드에 영향 없음)</li>\n<li>필드 삭제/타입 변경은 금지 → 새 필드를 추가하고 기존 필드는 deprecated 처리</li>\n<li>이벤트 타입(<code>event_type</code>)은 문자열이므로 새 타입 추가 시 스키마 변경 불필요</li>\n</ul>\n<p><strong>변경 프로세스:</strong></p>\n<ol>\n<li>Glue Data Catalog 스키마에 새 컬럼 추가</li>\n<li>Athena 테이블에 <code>ALTER TABLE ADD COLUMNS</code> 실행</li>\n<li>Redshift 테이블에 <code>ALTER TABLE ADD COLUMN</code> 실행</li>\n<li>Firehose는 스키마 변경을 자동으로 반영 (재시작 불필요)</li>\n</ol>\n<p>Parquet의 self-describing 특성 덕분에, 새 필드가 추가된 파일과 기존 파일이 공존해도 Athena가 정상적으로 쿼리할 수 있습니다. 기존 파일에서 새 필드는 NULL로 반환됩니다.</p>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"핵심-구현-3-athena와-redshift로-분석-자동화\">핵심 구현 3: Athena와 Redshift로 분석 자동화</h2>\n<h3 id=\"athena-테이블-생성\">Athena 테이블 생성</h3>\n<p><strong>Athena 쿼리 (즉시 조회):</strong></p>\n<pre><code class=\"language-sql\">-- 일별 구매 금액\nSELECT\n  DATE(from_unixtime(timestamp / 1000)) as date,\n  COUNT(*) as purchase_count,\n  SUM(amount) as total_amount\nFROM events\nWHERE event_type = 'purchase'\n  AND year = '2024' AND month = '05'\nGROUP BY DATE(from_unixtime(timestamp / 1000))\nORDER BY date DESC;</code></pre><p><strong>실행 시간:</strong></p>\n<pre><code class=\"language-\">Before (JSON):\n- 스캔 데이터: 10GB\n- 실행 시간: 50초\n- 비용: $0.05\n\nAfter (Parquet):\n- 스캔 데이터: 2GB\n- 실행 시간: 10초 (5배 향상)\n- 비용: $0.01 (80% 절감)</code></pre><h3 id=\"redshift-적재-배치\">Redshift 적재 (배치)</h3>\n<p><strong>30분 간격 증분 적재 (staging + merge):</strong></p>\n<pre class=\"mermaid\">graph LR\n    A[\"S3 Parquet\"] -->|COPY| B[\"Staging 테이블\"]\n    B -->|\"LEFT JOIN<br/>event_id 기준 dedup\"| C[\"Main 테이블\"]\n    C -->|완료| D[\"TRUNCATE Staging\"]</pre><pre><code class=\"language-sql\">-- 테이블 구조\nCREATE TABLE analytics.events (\n  event_id VARCHAR(64),\n  user_id BIGINT,\n  event_type VARCHAR(50),\n  product_id BIGINT,\n  amount DECIMAL(10,2),\n  timestamp TIMESTAMP,\n  server_timestamp TIMESTAMP\n)\nDISTKEY(user_id)\nSORTKEY(timestamp);</code></pre><pre><code class=\"language-sql\">-- Step 1: 당일 파티션을 스테이징 테이블에 COPY\nCOPY analytics.events_staging\nFROM 's3://my-events-bucket/events/year=2026/month=02/day=15/hour=14/'\nIAM_ROLE 'arn:aws:iam::123456789:role/RedshiftS3Role'\nFORMAT AS PARQUET;\n\n-- Step 2: event_id 기준으로 신규 데이터만 메인 테이블에 병합\nINSERT INTO analytics.events\nSELECT s.*\nFROM analytics.events_staging s\nLEFT JOIN analytics.events e ON s.event_id = e.event_id\nWHERE e.event_id IS NULL;\n\n-- Step 3: 스테이징 테이블 초기화\nTRUNCATE analytics.events_staging;</code></pre><p>S3의 Hive 스타일 파티셔닝(<code>year=/month=/day=/hour=</code>)은 Athena에서 자동으로 파티션으로 인식됩니다. Redshift에는 30분 간격으로 해당 시간 파티션을 스테이징 테이블에 COPY한 뒤, <code>event_id</code> 기준 dedup으로 신규 데이터만 메인 테이블에 병합합니다.</p>\n<h3 id=\"tableau-연동-및-최신-데이터-적재\">Tableau 연동 및 최신 데이터 적재</h3>\n<p><strong>Redshift 커넥터:</strong></p>\n<ul>\n<li>증분 적재 방식 사용 (비용 절감)</li>\n</ul>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"결과-데이터-리드타임-12일30분\">결과: 데이터 리드타임 1~2일→30분</h2>\n<h3 id=\"리드타임-단축\">리드타임 단축</h3>\n<table>\n<thead>\n<tr>\n<th>단계</th>\n<th>Before</th>\n<th>After</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>데이터 수집</td>\n<td>어드민 다운로드 / 개발팀 요청</td>\n<td>자동 (Firehose)</td>\n</tr>\n<tr>\n<td>전처리</td>\n<td>수동 (1시간)</td>\n<td>자동 (Firehose 네이티브 변환)</td>\n</tr>\n<tr>\n<td>적재</td>\n<td>수동 (1시간)</td>\n<td>자동 (S3)</td>\n</tr>\n<tr>\n<td><strong>리드타임</strong></td>\n<td><strong>1~2일</strong></td>\n<td><strong>30분</strong></td>\n</tr>\n</tbody></table>\n<p>기존에는 데이터 담당자가 어드민에서 직접 다운로드하거나 개발팀에 데이터를 요청한 뒤, 전처리를 거쳐 수동으로 적재해야 했습니다. 파이프라인 구축 후 이 과정이 완전히 자동화되었습니다.</p>\n<h3 id=\"데이터-품질-검증\">데이터 품질 검증</h3>\n<pre><code class=\"language-\">- 데이터 유실: 측정 기간 6개월간 0건\n- 검증 방법: API 전송 건수(CloudWatch PutRecord 메트릭) vs S3 적재 건수(Athena COUNT) 일일 비교\n- 에러 핸들링: Firehose가 변환 실패 레코드를 errors/ 프리픽스에 자동 분리 저장\n- 중복 처리: Firehose at-least-once 특성상 소수 중복 가능 → `event_id` 기준 dedup 뷰/집계로 정산 반영</code></pre><p>Firehose의 자동 재시도 메커니즘과 ErrorOutputPrefix 설정으로, 변환에 실패한 레코드도 별도 경로에 보관되어 원인 분석과 재처리가 가능합니다.</p>\n<h3 id=\"비즈니스-임팩트\">비즈니스 임팩트</h3>\n<table>\n<thead>\n<tr>\n<th>항목</th>\n<th>변화</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>마케팅팀 데이터 요청</strong></td>\n<td>주 5~10건 → 직접 Tableau 조회 (요청 자체가 불필요)</td>\n</tr>\n<tr>\n<td><strong>캠페인 의사결정 속도</strong></td>\n<td>캠페인 종료 후 1~2일 → 당일 오후 성과 확인 가능</td>\n</tr>\n<tr>\n<td><strong>개발자 시간 절약</strong></td>\n<td>월 40시간 (데이터 추출 작업 제거)</td>\n</tr>\n<tr>\n<td><strong>데이터 활용 범위</strong></td>\n<td>월간 리포트 → 일일 대시보드 + 실시간 모니터링</td>\n</tr>\n</tbody></table>\n<p>특히 마케팅팀이 <strong>캠페인 진행 중에 실시간으로 성과를 확인</strong>하고 타겟팅을 조정할 수 있게 되면서, 데이터 기반 의사결정의 속도가 근본적으로 달라졌습니다.</p>\n<h3 id=\"비용\">비용</h3>\n<table>\n<thead>\n<tr>\n<th>항목</th>\n<th>내용</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>추가 인프라 비용</strong></td>\n<td>월 약 10만원</td>\n</tr>\n<tr>\n<td><strong>스토리지 절감</strong></td>\n<td>Parquet 적용으로 70% 절감</td>\n</tr>\n</tbody></table>\n<h3 id=\"처리-성능\">처리 성능</h3>\n<pre><code class=\"language-\">- 월 평균 처리량: 3,000만 건\n- 월 피크 처리량: 5,000만 건\n- 일 평균: 100만 건</code></pre><hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"배운-점\">배운 점</h2>\n<p><strong>1. AWS 관리형 서비스의 위력</strong></p>\n<ul>\n<li>Firehose + Lambda + S3로 완전 자동화를 달성했습니다.</li>\n<li>서버 관리 불필요, 종량제 과금으로 비용을 최소화했습니다.</li>\n<li>비용 검증 필수</li>\n</ul>\n<p><strong>2. Parquet는 필수</strong></p>\n<ul>\n<li>스토리지 비용 70% 절감</li>\n<li>Athena 쿼리 속도 5배 향상</li>\n<li>압축은 Snappy (빠르고 적절한 압축률, Recommend)</li>\n</ul>\n<p><strong>3. 분석 레이어 이원화</strong></p>\n<ul>\n<li>Athena: 실시간 조회</li>\n<li>Redshift: 배치 적재 (30분 지연)</li>\n<li>용도에 따라 적절한 도구를 선택하는 것이 중요합니다.</li>\n</ul>\n<p><strong>4. 데이터 파티셔닝</strong></p>\n<pre><code class=\"language-\">s3://bucket/events/year=2024/month=05/day=08/hour=14/</code></pre><ul>\n<li>날짜별 파티션으로 쿼리 성능을 향상시켰습니다.</li>\n<li>불필요한 데이터 스캔을 방지하여 비용도 절감됩니다.</li>\n</ul>\n<p><strong>5. 히스토리 추적</strong></p>\n<ul>\n<li>S3 버전 관리로 데이터 복구가 가능합니다.</li>\n</ul>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"기술-스택\">기술 스택</h2>\n<table>\n<thead>\n<tr>\n<th>분류</th>\n<th>기술</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>이벤트 수집</strong></td>\n<td>AWS Data Firehose (서버리스)</td>\n</tr>\n<tr>\n<td><strong>데이터 변환</strong></td>\n<td>Firehose 네이티브 변환 (Glue Data Catalog)</td>\n</tr>\n<tr>\n<td><strong>스토리지</strong></td>\n<td>S3 (Parquet + Snappy)</td>\n</tr>\n<tr>\n<td><strong>쿼리 엔진</strong></td>\n<td>Athena (서버리스)</td>\n</tr>\n<tr>\n<td><strong>데이터 웨어하우스</strong></td>\n<td>Redshift (서버리스)</td>\n</tr>\n<tr>\n<td><strong>시각화</strong></td>\n<td>Tableau</td>\n</tr>\n</tbody></table>\n</section>\n"
  },
  {
    "id": "ecommerce-batch-optimization",
    "title": "e커머스 주문 수집 배치 성능 개선",
    "tags": [
      "Java",
      "Spring Boot",
      "Template Method",
      "CompletableFuture"
    ],
    "content": "<section class=\"content-section\">\n<h2 id=\"목차\">목차</h2>\n<ol>\n<li><a href=\"#%EB%B0%B0%EA%B2%BD-%EC%97%AC%EB%9F%AC-%EC%BB%A4%EB%A8%B8%EC%8A%A4%EC%9D%98-%EC%A3%BC%EB%AC%B8%EC%9D%84-%ED%95%9C-%EA%B3%B3%EC%97%90\">배경: 여러 커머스의 주문을 한 곳에</a></li>\n<li><a href=\"#%EB%AC%B8%EC%A0%9C-%EB%B6%84%EC%84%9D-%EC%88%9C%EC%B0%A8-%EC%B2%98%EB%A6%AC%EC%9D%98-%ED%95%9C%EA%B3%84\">문제 분석: 순차 처리의 한계</a></li>\n<li><a href=\"#%ED%95%B4%EA%B2%B0-%EB%AA%A9%ED%91%9C-%EC%B2%98%EB%A6%AC-%EC%8B%9C%EA%B0%84-10%EB%B0%B0-%EB%8B%A8%EC%B6%95\">해결 목표: 처리 시간 10배 단축</a></li>\n<li><a href=\"#%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98-%EC%84%A4%EA%B3%84-%ED%85%9C%ED%94%8C%EB%A6%BF-%EB%A9%94%EC%86%8C%EB%93%9C-%ED%8C%A8%ED%84%B4-%EC%84%A0%ED%83%9D-%EC%9D%B4%EC%9C%A0\">아키텍처 설계: 템플릿 메소드 패턴 선택 이유</a></li>\n<li><a href=\"#%ED%95%B5%EC%8B%AC-%EA%B5%AC%ED%98%84-1-%ED%85%9C%ED%94%8C%EB%A6%BF-%EB%A9%94%EC%86%8C%EB%93%9C-%ED%8C%A8%ED%84%B4%EC%9C%BC%EB%A1%9C-%EC%BD%94%EB%93%9C-%EC%A4%91%EB%B3%B5-%EC%A0%9C%EA%B1%B0\">핵심 구현 1: 템플릿 메소드 패턴으로 코드 중복 제거</a></li>\n<li><a href=\"#%ED%95%B5%EC%8B%AC-%EA%B5%AC%ED%98%84-2-completablefuture%EB%A1%9C-%ED%8C%90%EB%A7%A4%EC%9E%90%EB%B3%84-%EB%B3%91%EB%A0%AC-%EC%B2%98%EB%A6%AC\">핵심 구현 2: CompletableFuture로 판매자별 병렬 처리</a></li>\n<li><a href=\"#%ED%95%B5%EC%8B%AC-%EA%B5%AC%ED%98%84-3-%EB%B2%8C%ED%81%AC-%EC%B2%98%EB%A6%AC%EB%A1%9C-db-%EB%9D%BC%EC%9A%B4%EB%93%9C%ED%8A%B8%EB%A6%BD-%EC%B5%9C%EC%A0%81%ED%99%94\">핵심 구현 3: 벌크 처리로 DB 라운드트립 최적화</a></li>\n<li><a href=\"#%ED%95%B5%EC%8B%AC-%EA%B5%AC%ED%98%84-4-jenkins-%EA%B8%B0%EB%B0%98-%EC%9E%AC%EC%88%98%EC%A7%91-%EC%8B%9C%EC%8A%A4%ED%85%9C\">핵심 구현 4: Jenkins 기반 재수집 시스템</a></li>\n<li><a href=\"#%EA%B2%B0%EA%B3%BC-%EC%B2%98%EB%A6%AC-%EC%8B%9C%EA%B0%84-30%EB%B6%843%EB%B6%84-%EC%8B%A0%EA%B7%9C-%EC%BB%A4%EB%A8%B8%EC%8A%A4-%EC%B6%94%EA%B0%80-2%EC%A3%BC2%EC%9D%BC\">결과: 처리 시간 30분→3분, 신규 커머스 추가 2주→2일</a></li>\n</ol>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"배경-여러-커머스의-주문을-한-곳에\">배경: 여러 커머스의 주문을 한 곳에</h2>\n<p>우리 플랫폼은 여러 외부 커머스의 주문을 통합 관리합니다. 위메프, 롯데ON, TMON 등 5개 커머스에서 발생한 주문을 매시간 수집하여 우리 DB에 저장하고, 통합 대시보드로 제공합니다.</p>\n<p>각 커머스 배치는 <strong>여러 판매자의 API Key</strong>를 통해 주문 데이터를 수집합니다. 하나의 커머스에 수십 개의 판매자가 등록되어 있으며, 판매자마다 개별 API Key로 인증하여 주문을 조회합니다.</p>\n<p><strong>비즈니스 요구사항:</strong></p>\n<ul>\n<li>각 커머스의 주문 데이터를 1시간마다 수집</li>\n<li>통합 대시보드에서 실시간 현황 파악</li>\n<li>정산 데이터 집계 (커머스별 수수료 계산)</li>\n</ul>\n<p>하지만 판매자별로 순차 처리하다 보니 배치 하나에 30분이 걸렸고, 신규 커머스를 추가하려면 2주 이상 소요되었습니다.</p>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"문제-분석-순차-처리의-한계\">문제 분석: 순차 처리의 한계</h2>\n<h3 id=\"기존-시스템의-문제\">기존 시스템의 문제</h3>\n<p><strong>1. 판매자별 순차 처리로 인한 지연</strong></p>\n<p>하나의 커머스 배치 내에서 판매자(API Key)별로 순차 처리하는 구조였습니다.</p>\n<pre><code class=\"language-\">[위메프 배치]\n판매자 A 수집 (3분) → 판매자 B 수집 (2분) → 판매자 C 수집 (4분) → ...\n→ 전체 판매자 순차 처리 합계: 30분</code></pre><p>외부 API 응답 속도가 느린 데다 판매자 수가 많아, 순차 처리 시 시간이 크게 누적되었습니다.</p>\n<p><strong>2. 코드 중복 (70% 수준)</strong></p>\n<p>각 커머스별로 거의 동일한 로직을 중복 구현했습니다. <strong>각 커머스 수집 클래스가 1,000줄 이상</strong>의 코드를 포함하고 있었습니다.</p>\n<pre><code class=\"language-java\">// 위메프 배치\npublic void collectWemepOrders() {\n  for (String apiKey : wemepApiKeys) {\n    // 1. 인증\n    String token = authenticateWemep(apiKey);\n    // 2. API 호출\n    String response = callWemepAPI(token);\n    // 3. 파싱\n    List&lt;Order&gt; orders = parseWemepResponse(response);\n    // 4. 저장\n    saveOrders(orders);\n  }\n}\n\n// 롯데ON 배치 (거의 동일한 구조)\npublic void collectLotteOrders() {\n  for (String apiKey : lotteApiKeys) {\n    String token = authenticateLotte(apiKey);\n    String response = callLotteAPI(token);\n    List&lt;Order&gt; orders = parseLotteResponse(response);\n    saveOrders(orders);\n  }\n}\n\n// TMON, 쿠팡, 11번가... 계속 반복</code></pre><p><strong>3. 신규 커머스 추가 시간 (2주)</strong></p>\n<p>기존 배치 구조가 통일되어 있지 않아, 새로운 커머스를 추가하려면 처음부터 전체 로직을 구현해야 했습니다.</p>\n<ol>\n<li>API 문서 분석 (2일)</li>\n<li>인증 로직 구현 (1일)</li>\n<li>파싱 로직 구현 (3일)</li>\n<li>테스트 및 버그 수정 (5일)</li>\n<li>배포 (1일)</li>\n</ol>\n<p><strong>합계: 2주</strong></p>\n<p><strong>4. 개인정보 분리 DB로 인한 과다 DB 호출</strong></p>\n<p>주문 데이터는 <strong>분산 DB 구조</strong>로 저장됩니다. 개인정보 DB에 배송지 데이터를, 주문 DB에 주문 정보를 각각 저장하며, 두 테이블 간 FK 매핑이 필요합니다.</p>\n<pre><code class=\"language-\">주문 10,000건 처리 시:\n1. 개인정보 DB - 배송지 저장: 10,000번\n2. 주문 DB - 주문 정보 저장: 10,000번\n→ 총 20,000번의 개별 DB 저장</code></pre><p>이로 인해 DB 커넥션 풀이 고갈되고 전체 배치 처리 시간이 크게 증가했습니다.</p>\n<p><strong>5. 에러 전파</strong></p>\n<p>한 판매자에서 에러가 발생하면 해당 커머스 배치 전체가 중단되었습니다.</p>\n<pre><code class=\"language-\">판매자 A 성공 → 판매자 B 실패\n→ 판매자 C, D, E 수집 안 됨</code></pre><hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"해결-목표-처리-시간-10배-단축\">해결 목표: 처리 시간 10배 단축</h2>\n<h3 id=\"정량적-목표\">정량적 목표</h3>\n<ul>\n<li><strong>처리 시간</strong>: 30분 → 3분 (10배 단축)</li>\n<li><strong>코드 중복</strong>: 주요 수집 모듈 기준 60% 이상 감소</li>\n<li><strong>신규 커머스 추가</strong>: 2주 → 2일 (7배 단축)</li>\n<li><strong>DB 저장 단계</strong>: 주문당 2회 개별 저장 → 3단계 벌크 처리</li>\n</ul>\n<h3 id=\"정성적-목표\">정성적 목표</h3>\n<ul>\n<li>판매자별 병렬 처리로 처리 시간 최소화</li>\n<li>템플릿 메소드 패턴으로 코드 재사용성 극대화</li>\n<li>판매자별 독립적 에러 처리 (장애 격리)</li>\n<li>Jenkins 기반 재수집 시스템 (특정 기간 재처리)</li>\n</ul>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"아키텍처-설계-템플릿-메소드-패턴-선택-이유\">아키텍처 설계: 템플릿 메소드 패턴 선택 이유</h2>\n<h3 id=\"디자인-패턴-비교\">디자인 패턴 비교</h3>\n<table>\n<thead>\n<tr>\n<th>패턴</th>\n<th>장점</th>\n<th>단점</th>\n<th>적합성</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Strategy</strong></td>\n<td>런타임 교체 가능</td>\n<td>보일러플레이트 많음</td>\n<td>낮음</td>\n</tr>\n<tr>\n<td><strong>Factory</strong></td>\n<td>객체 생성 유연</td>\n<td>플로우 재사용 안 됨</td>\n<td>낮음</td>\n</tr>\n<tr>\n<td><strong>Template Method</strong></td>\n<td>플로우 재사용</td>\n<td>상속 필요</td>\n<td><strong>높음</strong></td>\n</tr>\n</tbody></table>\n<p><strong>템플릿 메소드 선택 이유:</strong></p>\n<ul>\n<li>모든 커머스가 동일한 플로우 (인증 → 조회 → 파싱 → 저장)</li>\n<li>각 단계별 구현만 다름 (인증 방식, 파싱 로직)</li>\n<li>상위 클래스에서 플로우 정의 → 하위 클래스는 구현만 담당</li>\n</ul>\n<h3 id=\"전체-아키텍처\">전체 아키텍처</h3>\n<pre class=\"mermaid\">graph TB\n    subgraph 커머스_배치[\"커머스 배치 (예: 위메프)\"]\n        A[배치 스케줄러] --> B[판매자 API Key 목록 로드]\n        B --> C{CompletableFuture<br/>병렬 실행}\n        C -->|Thread 1| D[판매자 A]\n        C -->|Thread 2| E[판매자 B]\n        C -->|Thread 3| F[판매자 C]\n        C -->|...| G[판매자 N]\n    end\n\n    subgraph 템플릿_메소드[\"CommerceCollector (템플릿 메소드)\"]\n        H[1. authenticate] --> I[2. fetchOrders]\n        I --> J[3. parseResponse]\n    end\n\n    D --> H\n    E --> H\n    F --> H\n    G --> H\n\n    subgraph 벌크_저장[\"벌크 저장 (분산 DB)\"]\n        J --> K[개인정보 DB<br/>배송지 벌크 INSERT]\n        K -->|FK 매핑<br/>UUID + Sequence| L[주문 DB<br/>주문 벌크 INSERT]\n    end</pre><hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"핵심-구현-1-템플릿-메소드-패턴으로-코드-중복-제거\">핵심 구현 1: 템플릿 메소드 패턴으로 코드 중복 제거</h2>\n<h3 id=\"추상-클래스-정의\">추상 클래스 정의</h3>\n<p>배치의 단계를 명확히 나누고, 커머스별로 상이한 부분만 추상화하여 개별 구현하도록 했습니다.</p>\n<p><strong>CommerceCollector (추상 클래스):</strong></p>\n<pre><code class=\"language-java\">public abstract class CommerceCollector {\n\n  // 템플릿 메소드 (플로우 정의)\n  public final void collect() {\n    // 1. 판매자 목록 로드\n    List&lt;String&gt; apiKeys = getApiKeys();\n\n    // 2. 판매자별 병렬 수집\n    List&lt;Order&gt; allOrders = collectFromSellers(apiKeys);\n\n    // 3. 벌크 저장 (공통 로직)\n    saveOrdersBulk(allOrders);\n\n    log.info(&quot;{} collected {} orders from {} sellers&quot;,\n      getName(), allOrders.size(), apiKeys.size());\n  }\n\n  // 하위 클래스에서 구현해야 하는 추상 메서드\n  protected abstract String authenticate(String apiKey);\n  protected abstract String fetchOrders(String token);\n  protected abstract List&lt;Order&gt; parseResponse(String response);\n\n  // 공통 로직 (하위 클래스에서 재사용)\n  protected abstract List&lt;String&gt; getApiKeys();\n  public abstract String getName();\n}</code></pre><h3 id=\"구체적-구현-위메프\">구체적 구현 (위메프)</h3>\n<p><strong>WemepCollector:</strong></p>\n<pre><code class=\"language-java\">@Component\npublic class WemepCollector extends CommerceCollector {\n\n  @Override\n  protected String authenticate(String apiKey) {\n    // OAuth 인증\n    return restTemplate.postForObject(\n      &quot;https://api.wemep.com/oauth/token&quot;,\n      new OAuth2Request(apiKey, clientSecret),\n      TokenResponse.class\n    ).getAccessToken();\n  }\n\n  @Override\n  protected String fetchOrders(String token) {\n    HttpHeaders headers = new HttpHeaders();\n    headers.setBearerAuth(token);\n\n    return restTemplate.exchange(\n      &quot;https://api.wemep.com/orders&quot;,\n      HttpMethod.GET,\n      new HttpEntity&lt;&gt;(headers),\n      String.class\n    ).getBody();\n  }\n\n  @Override\n  protected List&lt;Order&gt; parseResponse(String response) {\n    // JSON 파싱\n    WemepOrderResponse res = objectMapper.readValue(\n      response, WemepOrderResponse.class);\n\n    return res.getOrders().stream()\n      .map(this::transformToEntity)\n      .collect(Collectors.toList());\n  }\n\n  @Override\n  protected List&lt;String&gt; getApiKeys() {\n    return sellerRepository.findApiKeysByCommerce(&quot;WEMEP&quot;);\n  }\n\n  @Override\n  public String getName() {\n    return &quot;WEMEP&quot;;\n  }\n}</code></pre><h3 id=\"다른-커머스-구현-롯데on\">다른 커머스 구현 (롯데ON)</h3>\n<p><strong>LotteCollector:</strong></p>\n<pre><code class=\"language-java\">@Component\npublic class LotteCollector extends CommerceCollector {\n\n  @Override\n  protected String authenticate(String apiKey) {\n    // API Key 인증 (OAuth와 다름)\n    return apiKey;\n  }\n\n  @Override\n  protected String fetchOrders(String apiKey) {\n    // XML 응답 (JSON과 다름)\n    HttpHeaders headers = new HttpHeaders();\n    headers.set(&quot;X-API-KEY&quot;, apiKey);\n\n    return restTemplate.exchange(\n      &quot;https://api.lotteon.com/orders.xml&quot;,\n      HttpMethod.GET,\n      new HttpEntity&lt;&gt;(headers),\n      String.class\n    ).getBody();\n  }\n\n  @Override\n  protected List&lt;Order&gt; parseResponse(String response) {\n    // XML 파싱 (JSON과 다름)\n    Document doc = DocumentBuilderFactory.newInstance()\n      .newDocumentBuilder()\n      .parse(new InputSource(new StringReader(response)));\n\n    NodeList orderNodes = doc.getElementsByTagName(&quot;order&quot;);\n    List&lt;Order&gt; orders = new ArrayList&lt;&gt;();\n\n    for (int i = 0; i &lt; orderNodes.getLength(); i++) {\n      Element orderEl = (Element) orderNodes.item(i);\n      orders.add(transformToEntity(orderEl));\n    }\n\n    return orders;\n  }\n\n  @Override\n  protected List&lt;String&gt; getApiKeys() {\n    return sellerRepository.findApiKeysByCommerce(&quot;LOTTE&quot;);\n  }\n\n  @Override\n  public String getName() {\n    return &quot;LOTTE&quot;;\n  }\n}</code></pre><h3 id=\"코드-재사용-효과\">코드 재사용 효과</h3>\n<pre><code class=\"language-\">Before:\n- WemepCollector: 200줄\n- LotteCollector: 180줄\n- TmonCollector: 190줄\n- 합계: 570줄\n\nAfter:\n- CommerceCollector (추상): 50줄\n- WemepCollector: 60줄\n- LotteCollector: 50줄\n- TmonCollector: 55줄\n- 합계: 215줄 (62% 감소)</code></pre><hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"핵심-구현-2-completablefuture로-판매자별-병렬-처리\">핵심 구현 2: CompletableFuture로 판매자별 병렬 처리</h2>\n<h3 id=\"순차-처리-vs-병렬-처리\">순차 처리 vs 병렬 처리</h3>\n<p>기존에는 각 판매자(API Key)별로 순차적으로 데이터를 가져와서 적재하는 방식이었습니다.</p>\n<p><strong>Before (판매자별 순차):</strong></p>\n<pre><code class=\"language-java\">public List&lt;Order&gt; collectFromSellers(List&lt;String&gt; apiKeys) {\n  List&lt;Order&gt; allOrders = new ArrayList&lt;&gt;();\n\n  for (String apiKey : apiKeys) {\n    String token = authenticate(apiKey);      // 인증\n    String response = fetchOrders(token);     // API 호출 (느림)\n    List&lt;Order&gt; orders = parseResponse(response);\n    allOrders.addAll(orders);\n  }\n  // 판매자 20개 × 평균 1.5분 = 30분\n\n  return allOrders;\n}</code></pre><p><strong>After (판매자별 병렬):</strong></p>\n<p>스레드 풀은 Spring의 <code>TaskExecutor</code> 빈으로 관리합니다. 매 실행마다 스레드 풀을 생성/소멸하는 대신 Spring 컨테이너가 라이프사이클을 관리하도록 했습니다.</p>\n<pre><code class=\"language-java\">@Configuration\npublic class BatchConfig {\n  @Bean(&quot;commerceTaskExecutor&quot;)\n  public TaskExecutor commerceTaskExecutor() {\n    ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();\n    executor.setCorePoolSize(10);\n    executor.setMaxPoolSize(10);\n    executor.setThreadNamePrefix(&quot;commerce-&quot;);\n    executor.initialize();\n    return executor;\n  }\n}</code></pre><pre><code class=\"language-java\">@Autowired\n@Qualifier(&quot;commerceTaskExecutor&quot;)\nprivate TaskExecutor taskExecutor;\n\npublic List&lt;Order&gt; collectFromSellers(List&lt;String&gt; apiKeys) {\n  List&lt;CompletableFuture&lt;List&lt;Order&gt;&gt;&gt; futures = apiKeys.stream()\n    .map(apiKey -&gt; CompletableFuture.supplyAsync(\n      () -&gt; {\n        try {\n          String token = authenticate(apiKey);\n          String response = fetchOrders(token);\n          return parseResponse(response);\n        } catch (Exception e) {\n          log.error(&quot;Failed: {} - {}&quot;, getName(), apiKey, e);\n          return Collections.&lt;Order&gt;emptyList();\n        }\n      },\n      taskExecutor\n    ))\n    .collect(Collectors.toList());\n\n  // 모든 판매자 수집 완료 대기\n  List&lt;Order&gt; allOrders = futures.stream()\n    .map(CompletableFuture::join)\n    .flatMap(Collection::stream)\n    .collect(Collectors.toList());\n\n  // 판매자 20개 병렬 → 가장 느린 판매자 기준 약 2분\n  return allOrders;\n}</code></pre><h3 id=\"threadpool-크기-선택\">ThreadPool 크기 선택</h3>\n<table>\n<thead>\n<tr>\n<th>스레드 수</th>\n<th>장점</th>\n<th>단점</th>\n<th>선택</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>5개</td>\n<td>CPU 효율</td>\n<td>판매자 수 대비 적음</td>\n<td>-</td>\n</tr>\n<tr>\n<td><strong>10개</strong></td>\n<td><strong>판매자 수 대비 적절</strong></td>\n<td><strong>적당한 오버헤드</strong></td>\n<td><strong>선택</strong></td>\n</tr>\n<tr>\n<td>20개</td>\n<td>최대 병렬성</td>\n<td>메모리/컨텍스트 스위칭</td>\n<td>-</td>\n</tr>\n</tbody></table>\n<p><strong>10개 선택 이유:</strong></p>\n<ul>\n<li>커머스당 판매자 수가 10~20개 수준</li>\n<li>외부 API I/O 대기가 대부분이므로 CPU 바운드 아님</li>\n<li>향후 판매자 증가에도 충분한 여유</li>\n</ul>\n<h3 id=\"에러-복구-전략\">에러 복구 전략</h3>\n<p>판매자별 병렬 처리에서 실패 시 <code>emptyList()</code>를 반환하여 다른 판매자에 영향을 주지 않지만, 실패한 판매자의 주문이 유실될 수 있습니다. 이를 방지하기 위해 <strong>실패 기록 → 재시도 → 최종 알림</strong> 전략을 적용했습니다.</p>\n<pre><code class=\"language-java\">public List&lt;Order&gt; collectFromSellers(List&lt;String&gt; apiKeys) {\n  List&lt;CompletableFuture&lt;CollectResult&gt;&gt; futures = apiKeys.stream()\n    .map(apiKey -&gt; CompletableFuture.supplyAsync(\n      () -&gt; {\n        try {\n          String token = authenticate(apiKey);\n          String response = fetchOrders(token);\n          return CollectResult.success(apiKey, parseResponse(response));\n        } catch (Exception e) {\n          log.error(&quot;Failed: {} - {}&quot;, getName(), apiKey, e);\n          return CollectResult.failure(apiKey, e);\n        }\n      },\n      taskExecutor\n    ))\n    .collect(Collectors.toList());\n\n  List&lt;CollectResult&gt; results = futures.stream()\n    .map(CompletableFuture::join)\n    .collect(Collectors.toList());\n\n  // 실패한 판매자를 재시도 테이블에 기록\n  List&lt;CollectResult&gt; failures = results.stream()\n    .filter(CollectResult::isFailed)\n    .collect(Collectors.toList());\n\n  if (!failures.isEmpty()) {\n    retryRepository.saveAll(failures.stream()\n      .map(f -&gt; new RetryRecord(getName(), f.getApiKey(), LocalDateTime.now()))\n      .collect(Collectors.toList()));\n  }\n\n  return results.stream()\n    .filter(CollectResult::isSuccess)\n    .flatMap(r -&gt; r.getOrders().stream())\n    .collect(Collectors.toList());\n}</code></pre><p>배치는 5분 주기로 실행되며, 다음 배치에서 재시도 테이블을 확인하여 실패한 판매자의 주문을 우선 수집합니다. UPSERT 멱등성 덕분에 재시도 시에도 데이터 중복이 발생하지 않습니다. 3회 연속 실패 시 Slack 알림을 발송하여 운영팀이 개입할 수 있도록 했습니다.</p>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"핵심-구현-3-벌크-처리로-db-라운드트립-최적화\">핵심 구현 3: 벌크 처리로 DB 라운드트립 최적화</h2>\n<h3 id=\"분산-db-구조\">분산 DB 구조</h3>\n<p>주문 데이터는 개인정보 보호를 위해 분산 DB 구조로 저장됩니다.</p>\n<pre><code class=\"language-\">개인정보 DB: 배송지 데이터 (주소, 수령인 등)\n주문 DB: 주문 정보 (상품, 금액, 상태 등)</code></pre><p>주문을 저장하려면 먼저 개인정보 DB에 배송지를 저장하고, 채번된 배송지 ID(FK)를 주문 DB의 주문 데이터에 매핑해야 합니다.</p>\n<h3 id=\"문제-주문-10000건-db-라운드트립-20000번\">문제: 주문 10,000건 = DB 라운드트립 20,000번</h3>\n<p>기존 방식은 주문 하나를 처리할 때마다 2번의 DB I/O가 발생했습니다.</p>\n<pre><code class=\"language-java\">// 기존 방식 (주문 하나씩 처리)\nfor (Order order : orders) {\n  // 1. 개인정보 DB - 배송지 저장\n  Address address = personalInfoDb.save(order.getAddress());\n\n  // 2. 주문 DB - 주문 정보 저장 (배송지 FK 매핑)\n  order.setAddressId(address.getId());\n  orderDb.save(order);\n}\n\n// 주문 10,000건 → DB 라운드트립 20,000번</code></pre><h3 id=\"해결-3단계-벌크-저장으로-db-호출-구간-최소화\">해결: 3단계 벌크 저장으로 DB 호출 구간 최소화</h3>\n<pre><code class=\"language-java\">public void saveOrdersBulk(List&lt;Order&gt; orders) {\n  String batchUuid = UUID.randomUUID().toString();\n\n  // 1. 개인정보 DB - 배송지 벌크 저장 (1번의 DB I/O)\n  List&lt;Address&gt; addresses = orders.stream()\n    .map(Order::getAddress)\n    .collect(Collectors.toList());\n\n  for (int i = 0; i &lt; addresses.size(); i++) {\n    addresses.get(i).setBatchUuid(batchUuid);\n    addresses.get(i).setSequence(i);\n  }\n  personalInfoDb.saveAll(addresses);\n\n  // 2. 개인정보 DB - 저장된 배송지 조회 (1번의 DB I/O)\n  List&lt;Address&gt; savedAddresses = personalInfoDb\n    .findByBatchUuidOrderBySequence(batchUuid);\n\n  // 3. 주문 DB - FK 매핑 후 벌크 저장 (1번의 DB I/O)\n  for (int i = 0; i &lt; orders.size(); i++) {\n    orders.get(i).setAddressId(savedAddresses.get(i).getId());\n  }\n  orderDb.saveAll(orders);\n}\n\n// 주문 10,000건 처리도 &quot;저장 로직 기준 3단계&quot;로 단순화</code></pre><blockquote>\n<p>참고: ORM/JDBC 배치 설정(batch size)에 따라 실제 SQL 실행 횟수는 달라질 수 있으므로, 본 문서의 &quot;3&quot;은 물리 패킷 수가 아니라 <strong>저장 로직 단계 수</strong>를 의미합니다.</p>\n</blockquote>\n<h3 id=\"동시성-문제와-해결\">동시성 문제와 해결</h3>\n<p><strong>문제 발생:</strong></p>\n<p>여러 커머스 배치가 동시에 실행되므로, 개인정보 DB에 동시에 배송지를 삽입합니다. 기존에는 마지막으로 삽입된 ID 기준으로 데이터를 1씩 증가시켜 매핑했지만, 동시성 이슈가 발생했습니다.</p>\n<pre><code class=\"language-java\">// 문제가 있는 코드\n// 1. 배송지 20개 벌크 저장\npersonalInfoDb.saveAll(addresses);\n\n// 2. 마지막 ID 조회\nLong lastId = personalInfoDb.findMaxId(); // 520\n\n// 3. ID 범위 계산 (동시성 문제!)\n// 500번부터 520번까지가 이번 배치라고 가정\nList&lt;Long&gt; ids = IntStream.rangeClosed(lastId - 20, lastId)\n  .boxed()\n  .collect(Collectors.toList());</code></pre><p><strong>문제 상황:</strong></p>\n<pre><code class=\"language-\">위메프 배치: 배송지 20개 저장 → 마지막 ID: 520\n롯데ON 배치: 동시에 배송지 15개 저장 → 마지막 ID: 535\n위메프 배치: 마지막 ID 조회 → 535를 가져옴\n위메프 배치: 515~535를 자신의 배송지로 착각 (잘못된 매핑)</code></pre><p><strong>해결: UUID + Sequence</strong></p>\n<pre class=\"mermaid\">sequenceDiagram\n    participant W as 위메프 배치\n    participant DB as 개인정보 DB\n    participant L as 롯데ON 배치\n\n    W->>DB: saveAll(20건, batchUuid=A, seq=0~19)\n    L->>DB: saveAll(15건, batchUuid=B, seq=0~14)\n    Note over W,L: 동시 INSERT — ID 범위가 섞여도 무관\n    W->>DB: findByBatchUuid('A') ORDER BY seq\n    DB-->>W: 정확히 20건 (순서 보장)\n    L->>DB: findByBatchUuid('B') ORDER BY seq\n    DB-->>L: 정확히 15건 (순서 보장)\n    Note over W,L: UUID 격리로 동시성 안전 + Sequence로 순서 보장</pre><pre><code class=\"language-java\">// 배치 작업별 UUID 생성\nString batchUuid = UUID.randomUUID().toString();\n\n// 1. UUID와 Sequence를 함께 저장\nfor (int i = 0; i &lt; addresses.size(); i++) {\n  addresses.get(i).setBatchUuid(batchUuid);\n  addresses.get(i).setSequence(i); // 순서 보장\n}\npersonalInfoDb.saveAll(addresses);\n\n// 2. UUID로 조회 (동시성 안전)\nList&lt;Address&gt; savedAddresses = personalInfoDb\n  .findByBatchUuidOrderBySequence(batchUuid);</code></pre><p><strong>Sequence가 필요한 이유:</strong></p>\n<p>벌크 INSERT 시 삽입 순서와 조회 순서가 일치한다는 보장이 없습니다.</p>\n<pre><code class=\"language-sql\">INSERT INTO addresses VALUES (...), (...), (...);\n-- 저장 순서: 1, 2, 3 (예상)\n-- 실제 순서: 2, 1, 3 (가능)</code></pre><p>Sequence를 함께 저장하면 정확한 순서로 조회할 수 있습니다.</p>\n<pre><code class=\"language-sql\">SELECT * FROM addresses\nWHERE batch_uuid = '...'\nORDER BY sequence ASC;</code></pre><hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"핵심-구현-4-jenkins-기반-재수집-시스템\">핵심 구현 4: Jenkins 기반 재수집 시스템</h2>\n<h3 id=\"jenkins-파라미터화-배치\">Jenkins 파라미터화 배치</h3>\n<p>특정 기간, 특정 커머스의 주문을 재수집할 수 있는 시스템을 구축했습니다.</p>\n<p><strong>Jenkinsfile:</strong></p>\n<pre><code class=\"language-groovy\">pipeline {\n  agent any\n\n  parameters {\n    string(\n      name: 'START_DATE',\n      defaultValue: '2024-05-01',\n      description: '수집 시작 날짜 (yyyy-MM-dd)'\n    )\n    string(\n      name: 'END_DATE',\n      defaultValue: '2024-05-08',\n      description: '수집 종료 날짜 (yyyy-MM-dd)'\n    )\n    choice(\n      name: 'COMMERCE',\n      choices: ['ALL', 'WEMEP', 'LOTTE', 'TMON', 'COUPANG', '11ST'],\n      description: '재수집할 커머스 선택'\n    )\n  }\n\n  stages {\n    stage('Re-collect Orders') {\n      steps {\n        sh &quot;&quot;&quot;\n          java -jar batch.jar \\\n            --job=reCollectOrders \\\n            --startDate=${params.START_DATE} \\\n            --endDate=${params.END_DATE} \\\n            --commerce=${params.COMMERCE}\n        &quot;&quot;&quot;\n      }\n    }\n  }\n\n  post {\n    failure {\n      slackSend(\n        channel: '#batch-alerts',\n        color: 'danger',\n        message: &quot;재수집 실패: ${params.COMMERCE} (${params.START_DATE} ~ ${params.END_DATE})&quot;\n      )\n    }\n  }\n}</code></pre><h3 id=\"재수집-로직-중복-방지\">재수집 로직 (중복 방지)</h3>\n<p><strong>UPSERT 쿼리:</strong></p>\n<pre><code class=\"language-sql\">INSERT INTO orders (\n  order_id, commerce, amount, status, created_at, updated_at\n)\nVALUES (?, ?, ?, ?, ?, NOW())\nON DUPLICATE KEY UPDATE\n  amount = VALUES(amount),\n  status = VALUES(status),\n  updated_at = NOW();</code></pre><ul>\n<li>Primary Key: <code>(order_id, commerce)</code></li>\n<li>같은 주문이 들어오면 UPDATE, 새로운 주문이면 INSERT</li>\n<li>재수집 시에도 데이터 중복이 발생하지 않습니다.</li>\n</ul>\n<h3 id=\"slack-알림-연동\">Slack 알림 연동</h3>\n<pre><code class=\"language-java\">@Component\npublic class SlackNotifier {\n\n  public void notifyBatchFailure(String commerce, Exception e) {\n    String message = String.format(\n      &quot;:x: *배치 실패*\\n&quot; +\n      &quot;커머스: %s\\n&quot; +\n      &quot;에러: %s\\n&quot; +\n      &quot;시간: %s\\n&quot; +\n      &quot;&lt;https://jenkins.example.com|재실행&gt;&quot;,\n      commerce,\n      e.getMessage(),\n      LocalDateTime.now()\n    );\n\n    slackClient.postMessage(&quot;#batch-alerts&quot;, message);\n  }\n}</code></pre><hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"결과-처리-시간-30분3분-신규-커머스-추가-2주2일\">결과: 처리 시간 30분→3분, 신규 커머스 추가 2주→2일</h2>\n<h3 id=\"성능-개선\">성능 개선</h3>\n<table>\n<thead>\n<tr>\n<th>지표</th>\n<th>Before</th>\n<th>After</th>\n<th>개선률</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>처리 시간</strong></td>\n<td>30분</td>\n<td>3분</td>\n<td><strong>10배 단축</strong></td>\n</tr>\n<tr>\n<td><strong>코드 중복(주요 모듈 코드 라인)</strong></td>\n<td>570줄</td>\n<td>215줄</td>\n<td><strong>62% 감소</strong></td>\n</tr>\n<tr>\n<td><strong>신규 커머스 추가</strong></td>\n<td>2주</td>\n<td>2일</td>\n<td><strong>7배 단축</strong></td>\n</tr>\n<tr>\n<td><strong>DB 저장 방식</strong></td>\n<td>주문당 개별 저장(2회)</td>\n<td>3단계 벌크 처리</td>\n<td><strong>호출 구간 대폭 단순화</strong></td>\n</tr>\n</tbody></table>\n<h3 id=\"개선-경로\">개선 경로</h3>\n<p>처리 시간 단축은 두 단계에 걸쳐 이루어졌습니다.</p>\n<pre><code class=\"language-\">1단계 - 판매자별 병렬 처리 (CompletableFuture):\n  30분 → 가장 느린 판매자 기준으로 단축\n\n2단계 - 벌크 DB 처리:\n  개별 DB I/O 중심 저장 → 3단계 벌크 저장으로 전환\n  DB 커넥션 대기 시간 대폭 단축\n\n최종: 30분 → 3분 (10배)</code></pre><h3 id=\"확장성\">확장성</h3>\n<p><strong>신규 커머스 추가 시간:</strong></p>\n<pre><code class=\"language-\">Before: 2주\n1. API 문서 분석 (2일)\n2. 전체 로직 구현 (5일)\n3. 테스트 (5일)\n4. 배포 (1일)\n\nAfter: 2일\n1. API 문서 분석 (1일)\n2. 3개 메서드만 구현 (4시간)\n   - authenticate()\n   - parseResponse()\n   - transformToEntity()\n3. 테스트 (4시간)</code></pre><h3 id=\"장애-격리\">장애 격리</h3>\n<p><strong>Before:</strong></p>\n<pre><code class=\"language-\">판매자 B 실패 → 해당 커머스 배치 전체 중단\n나머지 판매자 수집 안 됨</code></pre><p><strong>After:</strong></p>\n<pre><code class=\"language-\">판매자 B 실패 → 해당 판매자만 건너뜀\n나머지 판매자는 정상 수집</code></pre><hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"배운-점\">배운 점</h2>\n<p><strong>1. 디자인 패턴은 실용적으로</strong></p>\n<ul>\n<li>템플릿 메소드 패턴이 완벽히 맞는 상황이었습니다.</li>\n<li>배치 플로우의 통일감과 유지보수성을 동시에 달성했습니다.</li>\n<li>신규 개발자도 추상 메서드만 구현하면 되어 학습 비용이 낮습니다.</li>\n</ul>\n<p><strong>2. CompletableFuture는 I/O 바운드 병렬 처리에 최적</strong></p>\n<ul>\n<li>외부 API 호출이 대부분인 작업은 병렬화 효과가 큽니다.</li>\n<li>판매자별 독립 실행으로 장애 격리도 자연스럽게 달성됩니다.</li>\n</ul>\n<p><strong>3. 분산 DB 간 FK 매핑은 UUID로</strong></p>\n<ul>\n<li>Auto Increment ID 기반 매핑은 동시성 환경에서 위험합니다.</li>\n<li>UUID + Sequence 조합으로 정확한 매핑과 순서 보장을 달성했습니다.</li>\n</ul>\n<p><strong>4. Jenkins + Slack 조합</strong></p>\n<ul>\n<li>실패 시 즉시 알림이 오고, Jenkins에서 클릭 한 번으로 재실행할 수 있습니다.</li>\n<li>UPSERT로 멱등성을 보장하여 재수집 시에도 데이터 정합성이 유지됩니다.</li>\n</ul>\n<p><strong>5. 벌크 처리는 배치의 핵심</strong></p>\n<ul>\n<li>개별 INSERT 대비 DB 트랜잭션 횟수를 대폭 감소시킵니다.</li>\n<li>분산 DB 환경에서는 FK 매핑 전략이 특히 중요합니다.</li>\n</ul>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"기술-스택\">기술 스택</h2>\n<table>\n<thead>\n<tr>\n<th>분류</th>\n<th>기술</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>언어</strong></td>\n<td>Java</td>\n</tr>\n<tr>\n<td><strong>프레임워크</strong></td>\n<td>Spring Boot</td>\n</tr>\n<tr>\n<td><strong>비동기 처리</strong></td>\n<td>CompletableFuture</td>\n</tr>\n<tr>\n<td><strong>디자인 패턴</strong></td>\n<td>Template Method Pattern</td>\n</tr>\n<tr>\n<td><strong>데이터베이스</strong></td>\n<td>MySQL (분산 DB: 개인정보 DB + 주문 DB)</td>\n</tr>\n<tr>\n<td><strong>CI/CD</strong></td>\n<td>Jenkins (파라미터화 배치)</td>\n</tr>\n<tr>\n<td><strong>알림</strong></td>\n<td>Slack Webhook</td>\n</tr>\n</tbody></table>\n</section>\n"
  },
  {
    "id": "image-optimization-lambda-edge",
    "title": "실시간 이미지 최적화 시스템",
    "tags": [
      "Lambda@Edge",
      "CloudFront",
      "S3",
      "WebP",
      "AVIF"
    ],
    "content": "<section class=\"content-section\">\n<h2 id=\"목차\">목차</h2>\n<ol>\n<li><a href=\"#%EB%B0%B0%EA%B2%BD-%ED%8A%B8%EB%9E%98%ED%94%BD-30-%EC%A6%9D%EA%B0%80-%EB%B9%84%EC%9A%A9%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C\">배경: 트래픽 30% 증가, 비용은 어떻게?</a></li>\n<li><a href=\"#%EB%AC%B8%EC%A0%9C-%EB%B6%84%EC%84%9D-%EC%9B%90%EB%B3%B8-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EC%84%9C%EB%B9%99%EC%9D%98-%EB%82%AD%EB%B9%84\">문제 분석: 원본 이미지 서빙의 낭비</a></li>\n<li><a href=\"#%ED%95%B4%EA%B2%B0-%EB%AA%A9%ED%91%9C-%EB%B9%84%EC%9A%A9-%EC%A0%88%EA%B0%90%EA%B3%BC-%EC%86%8D%EB%8F%84-%EA%B0%9C%EC%84%A0\">해결 목표: 비용 절감과 속도 개선</a></li>\n<li><a href=\"#%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98-%EC%84%A4%EA%B3%84-lambdaedge-%EC%84%A0%ED%83%9D-%EC%9D%B4%EC%9C%A0\">아키텍처 설계: Lambda@Edge 선택 이유</a></li>\n<li><a href=\"#%ED%95%B5%EC%8B%AC-%EA%B5%AC%ED%98%84-1-cloudfront-functions%EB%A1%9C-%EC%96%B4%EB%B7%B0%EC%A7%95-%EB%B0%A9%EC%A7%80\">핵심 구현 1: CloudFront Functions로 어뷰징 방지</a></li>\n<li><a href=\"#%ED%95%B5%EC%8B%AC-%EA%B5%AC%ED%98%84-2-lambdaedge%EB%A1%9C-on-demand-%EC%B2%98%EB%A6%AC\">핵심 구현 2: Lambda@Edge로 On-demand 처리</a></li>\n<li><a href=\"#%ED%95%B5%EC%8B%AC-%EA%B5%AC%ED%98%84-3-%EC%BA%90%EC%8B%9C-%EB%AC%B4%ED%9A%A8%ED%99%94-%EC%A0%84%EB%9E%B5\">핵심 구현 3: 캐시 무효화 전략</a></li>\n<li><a href=\"#%EA%B2%B0%EA%B3%BC-%EB%B9%84%EC%9A%A9-75-%EC%A0%88%EA%B0%90-%EC%86%8D%EB%8F%84-50-%EA%B0%9C%EC%84%A0\">결과: 비용 75% 절감, 속도 50% 개선</a></li>\n<li><a href=\"#%ED%8A%B8%EB%9F%AC%EB%B8%94%EC%8A%88%ED%8C%85\">트러블슈팅</a></li>\n</ol>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"배경-트래픽-30-증가-비용은-어떻게\">배경: 트래픽 30% 증가, 비용은 어떻게?</h2>\n<p>에브리타임은 대학생 커뮤니티 앱입니다. 2024년 3월, 우리 커머스 서비스가 앱 내 전용 &#39;혜택탭&#39;으로 노출되기 시작했습니다.</p>\n<img src=\"images/에브리타임_혜택탭이미지.png\" alt=\"에브리타임 혜택탭\" width=\"300\">\n\n<p><strong>변화의 규모:</strong></p>\n<ul>\n<li>거의 모든 페이지에 배너와 상품 이미지 노출</li>\n<li>출시 후 트래픽 30% 급증</li>\n<li>CloudFront 비용이 지속적으로 증가하는 추세</li>\n</ul>\n<p><strong>실제 CloudFront 비용 추이:</strong></p>\n<pre><code class=\"language-\">2024년 1월: 기준점\n2024년 2월: 20% 증가\n2024년 3-12월: 평균 유지\n2025년 1월: 전년 대비 24.6% 추가 증가</code></pre><p>월별 데이터 전송량도 400GB에서 12,600GB까지 증가.</p>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"문제-분석-원본-이미지-서빙의-낭비\">문제 분석: 원본 이미지 서빙의 낭비</h2>\n<h3 id=\"기존-시스템의-문제\">기존 시스템의 문제</h3>\n<p><strong>1. 원본 이미지를 그대로 전송</strong></p>\n<pre><code class=\"language-\">- 업로드: 1920x1080 (300KB) 이미지\n- 실제 사용: 400x300 크기로 리사이징해서 표시\n- 전송: 300KB 전체를 다운로드 😱\n- 낭비: 200KB (66%)</code></pre><p><strong>2. 서비스별 요구 크기가 제각각</strong></p>\n<ul>\n<li>썸네일: 200x200</li>\n<li>리스트뷰: 400x300</li>\n<li>상세페이지: 800x600</li>\n<li>배너: 1200x400</li>\n</ul>\n<p>하지만 시스템은 모든 경우에 동일한 원본 이미지를 전송했습니다.</p>\n<p><strong>3. 차세대 포맷 미지원</strong></p>\n<pre><code class=\"language-\">- JPEG (100KB)\n- WebP (60KB) - 40% 절감\n- AVIF (45KB) - 55% 절감</code></pre><p>JPEG만 지원했기 때문에 WebP, AVIF를 지원하는 모던 브라우저에서도 큰 파일을 다운로드했습니다.</p>\n<h3 id=\"poc-실제-압축률-측정\">PoC: 실제 압축률 측정</h3>\n<p>실제 프로덕션 이미지로 측정한 결과:</p>\n<p><strong>테스트 대상: 상품 상세 이미지 (442px 사용 중)</strong></p>\n<ul>\n<li>원본: JPEG, 35.1KB</li>\n</ul>\n<p><strong>포맷만 변경:</strong></p>\n<pre><code class=\"language-\">WebP: 9.9KB (71.7% 감소)\nAVIF: 5.2KB (85.2% 감소)</code></pre><p><strong>포맷 + 사이즈 변경 (442px):</strong></p>\n<pre><code class=\"language-\">WebP: 5.4KB (84.6% 감소)\nAVIF: 3.3KB (90.6% 감소)</code></pre><p><strong>결론:</strong> AVIF가 압축률은 최고지만, Opera Mini 등 일부 브라우저에서 미지원. WebP는 IE를 제외한 모든 브라우저에서 지원하므로 <strong>WebP를 기본으로 선택하되, <code>&lt;picture&gt;</code> 태그로 AVIF도 지원</strong>하는 방향을 채택했습니다.</p>\n<p><strong>4. 클라이언트 단에서 크롭</strong></p>\n<pre><code class=\"language-html\">&lt;img src=&quot;original.jpg&quot; style=&quot;width: 200px; height: 200px; object-fit: cover;&quot;&gt;</code></pre><p>이미지는 300KB를 전부 다운로드하고, 브라우저에서 200x200으로 잘라서 표시. UX 저하 + 대역폭 낭비.</p>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"해결-목표-비용-절감과-속도-개선\">해결 목표: 비용 절감과 속도 개선</h2>\n<h3 id=\"정량적-목표\">정량적 목표</h3>\n<ul>\n<li><strong>CloudFront 비용</strong>: 50% 이상 절감</li>\n<li><strong>페이지 로딩 속도</strong>: 50% 개선</li>\n<li><strong>Lambda 타임아웃</strong>: 85% 감소 (20% → 3%)</li>\n</ul>\n<h3 id=\"정성적-목표\">정성적 목표</h3>\n<ul>\n<li>On-demand 이미지 처리로 다양한 크기 요구사항 대응</li>\n<li>차세대 포맷(WebP, AVIF) 자동 지원</li>\n<li>악의적 요청 차단으로 비용 폭증 방지</li>\n<li>기존 URL 구조 유지 (호환성 보장)</li>\n</ul>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"아키텍처-설계-lambdaedge-선택-이유\">아키텍처 설계: Lambda@Edge 선택 이유</h2>\n<h3 id=\"lambdaedge-vs-ec2-vs-ecs\">Lambda@Edge vs EC2 vs ECS</h3>\n<p>이미지 리사이징 시스템을 구축할 때 고려한 3가지 옵션:</p>\n<table>\n<thead>\n<tr>\n<th>기준</th>\n<th>EC2/ECS</th>\n<th>Lambda@Edge</th>\n<th>S3 Pre-processing</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>비용</td>\n<td>상시 과금</td>\n<td>요청당 과금</td>\n<td>저장 공간 과다</td>\n</tr>\n<tr>\n<td>확장성</td>\n<td>수동 스케일링</td>\n<td>자동 무한 확장</td>\n<td>사전 생성 필요</td>\n</tr>\n<tr>\n<td>지연시간</td>\n<td>Origin까지 왕복</td>\n<td>Edge에서 처리</td>\n<td>빠름</td>\n</tr>\n<tr>\n<td>유지보수</td>\n<td>서버 관리 필요</td>\n<td>완전 관리형</td>\n<td>스크립트 관리</td>\n</tr>\n<tr>\n<td>유연성</td>\n<td>높음</td>\n<td>중간</td>\n<td>낮음</td>\n</tr>\n</tbody></table>\n<p><strong>Lambda@Edge를 선택한 이유:</strong></p>\n<ul>\n<li><strong>비용 효율</strong>: 트래픽이 적을 때는 비용 거의 0원</li>\n<li><strong>글로벌 배포</strong>: 전 세계 Edge Location에서 실행</li>\n<li><strong>완전 관리형</strong>: 서버 관리 불필요</li>\n<li><strong>캐시 통합</strong>: CloudFront와 네이티브 통합</li>\n</ul>\n<h3 id=\"전체-아키텍처\">전체 아키텍처</h3>\n<p><img src=\"images/Cloudfront_architecture.png\" alt=\"이미지 최적화 아키텍처\"></p>\n<h3 id=\"요청-처리-흐름\">요청 처리 흐름</h3>\n<pre class=\"mermaid\">sequenceDiagram\n    participant Client as 브라우저\n    participant CF_Func as CF Functions<br/>(Viewer Request)\n    participant CF as CloudFront<br/>(캐시)\n    participant Lambda as Lambda@Edge<br/>(Origin Response)\n    participant S3 as S3\n\n    Client->>CF_Func: ?w=387&h=287&f=webp&q=75\n    CF_Func->>CF_Func: 검증 / 10px 올림 정규화 / 쿼리 정렬\n    CF_Func->>CF: ?f=webp&h=290&q=80&w=390\n\n    alt 캐시 HIT\n        CF-->>Client: 캐시된 이미지 응답 (Lambda 실행 없음)\n    else 캐시 MISS\n        CF->>S3: 원본 이미지 요청\n        S3-->>CF: 원본 이미지 반환\n        CF->>Lambda: Origin Response 트리거\n        Lambda->>Lambda: 이미지 리사이징 + 포맷 변환\n        Lambda-->>CF: 최적화된 이미지\n        CF->>CF: 캐시 저장\n        CF-->>Client: 최적화된 이미지 응답\n    end</pre><h3 id=\"이벤트-트리거-선택-이유\">이벤트 트리거 선택 이유</h3>\n<p>CloudFront는 4가지 이벤트 트리거를 제공합니다. 각 트리거의 실행 시점과 역할이 다르기 때문에, 비용과 기능 요구사항에 맞는 배치가 중요합니다.</p>\n<table>\n<thead>\n<tr>\n<th>트리거</th>\n<th>실행 시점</th>\n<th>실행 빈도</th>\n<th>사용 가능 서비스</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Viewer Request</strong></td>\n<td>요청 수신 직후 (캐시 확인 전)</td>\n<td>모든 요청</td>\n<td>CF Functions, Lambda@Edge</td>\n</tr>\n<tr>\n<td>Viewer Response</td>\n<td>응답 반환 직전</td>\n<td>모든 요청</td>\n<td>CF Functions, Lambda@Edge</td>\n</tr>\n<tr>\n<td>Origin Request</td>\n<td>캐시 MISS → Origin 전달 시</td>\n<td>캐시 MISS만</td>\n<td>Lambda@Edge</td>\n</tr>\n<tr>\n<td><strong>Origin Response</strong></td>\n<td>Origin 응답 수신 후 (캐시 저장 전)</td>\n<td>캐시 MISS만</td>\n<td>Lambda@Edge</td>\n</tr>\n</tbody></table>\n<h3 id=\"url-파라미터-설계\">URL 파라미터 설계</h3>\n<pre><code class=\"language-\">https://cdn.example.com/products/123.jpg?w=400&amp;h=300&amp;f=webp&amp;q=80\n\n- w: width (너비)\n- h: height (높이)\n- f: format (webp, avif, jpg, png)\n- q: quality (1-100, 기본 80)</code></pre><hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"핵심-구현-1-cloudfront-functions로-어뷰징-방지\">핵심 구현 1: CloudFront Functions로 어뷰징 방지</h2>\n<h3 id=\"문제-악의적-요청으로-비용-폭증\">문제: 악의적 요청으로 비용 폭증</h3>\n<p>Lambda@Edge는 요청당 과금입니다. 악의적인 사용자가 다음과 같은 요청을 보내면?</p>\n<pre><code class=\"language-\">?w=1&amp;h=1    // 1x1 픽셀\n?w=2&amp;h=2    // 2x2 픽셀\n?w=3&amp;h=3    // 3x3 픽셀\n...\n?w=5000&amp;h=5000  // 5000x5000 픽셀</code></pre><p>각각 Lambda 실행 → 캐시되지 않음 → 비용 폭증 💸</p>\n<h3 id=\"해결-cloudfront-functions로-검증정규화\">해결: CloudFront Functions로 검증/정규화</h3>\n<p>검증/정규화 로직은 Lambda@Edge에서도 구현할 수 있지만, <strong>CloudFront Functions를 별도로 분리</strong>했습니다.</p>\n<table>\n<thead>\n<tr>\n<th>기준</th>\n<th>CloudFront Functions</th>\n<th>Lambda@Edge</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>사용 가능 트리거</strong></td>\n<td>Viewer Request/Response</td>\n<td>4개 전부 (Viewer/Origin × Request/Response)</td>\n</tr>\n<tr>\n<td><strong>실행 비용</strong></td>\n<td>요청 100만 건당 $0.10</td>\n<td>요청 100만 건당 $0.60</td>\n</tr>\n<tr>\n<td><strong>실행 속도</strong></td>\n<td>&lt; 1ms</td>\n<td>5ms ~ 수 초</td>\n</tr>\n<tr>\n<td><strong>제한</strong></td>\n<td>네트워크/파일 I/O 불가, 최대 10KB</td>\n<td>제한 없음</td>\n</tr>\n</tbody></table>\n<p>Lambda@Edge도 Viewer Request에서 동일한 검증 로직을 실행할 수 있지만, 검증/정규화는 네트워크 I/O가 필요 없는 단순 연산이므로 CF Functions의 제한에 해당하지 않습니다. <strong>모든 요청에서 실행</strong>되는 Viewer Request 특성상, 비용이 1/6이고 속도가 10배 빠른 CF Functions가 적합합니다.</p>\n<p>또한 정규화는 반드시 <strong>캐시 확인 전</strong>(Viewer Request)에 실행되어야 합니다. 만약 Origin Request에서 처리했다면, 정규화 전 쿼리로 캐시 키가 생성되어 동일한 이미지 요청도 캐시를 활용하지 못하게 됩니다.</p>\n<p><strong>정규화 로직:</strong></p>\n<pre><code class=\"language-javascript\">function handler(event) {\n  var params = event.request.querystring;\n\n  // 1. w, h: 범위 제한(50~2000) + 10px 단위 올림\n  //    예: w=387 → w=390, w=5000 → w=2000\n  normalize(params.w, MIN=50, MAX=2000, STEP=10);\n  normalize(params.h, MIN=50, MAX=2000, STEP=10);\n\n  // 2. q: 품질 범위 제한(10~100) + 10단위 올림\n  normalize(params.q, MIN=10, MAX=100, STEP=10);\n\n  // 3. f: 허용 포맷만 통과 (jpeg, png, webp, avif, gif, svg)\n  validateFormat(params.f);\n\n  // 4. 쿼리 키 정렬 → 동일 파라미터 조합이 같은 캐시 키로 매핑\n  //    ?w=390&amp;f=webp → ?f=webp&amp;w=390\n  sortQueryString(params);\n\n  return request;\n}</code></pre><p><strong>효과:</strong></p>\n<ul>\n<li><code>?w=387&amp;h=287</code> → <code>?w=390&amp;h=290</code> (10px 단위 정규화)</li>\n<li><code>?w=755</code> → <code>?w=760</code> (올림)</li>\n<li>캐시 키 개수: 1,950개 → 195개 (<strong>10배 감소</strong>)</li>\n</ul>\n<p><strong>10px 단위 선택 이유:</strong></p>\n<p>정규화 단위를 결정할 때 100px과 10px을 비교했습니다.</p>\n<ul>\n<li><strong>100px 단위의 한계</strong>: 서비스 요구사항 상 100px 단위로 떨어지지 않는 이미지가 다수 존재 (예: 썸네일 142px, 리스트뷰 342px, 배너 768px). 반올림 시 원본과 크기 차이가 눈에 띄는 경우 발생</li>\n<li><strong>10px 단위 채택</strong>: 실제 서비스에서 10px 단위로 떨어지는 이미지 규격이 많아 자연스럽게 정확한 크기 매칭 가능</li>\n<li>품질 저하 없이 캐시 효율을 확보하는 현실적인 균형점</li>\n</ul>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"핵심-구현-2-lambdaedge로-on-demand-처리\">핵심 구현 2: Lambda@Edge로 On-demand 처리</h2>\n<p>이미지 리사이징과 포맷 변환은 Node.js 기반 이미지 처리 라이브러리인 <a href=\"https://github.com/lovell/sharp\">Sharp</a>를 활용하여 구현했습니다.</p>\n<pre><code class=\"language-typescript\">import { S3Client, GetObjectCommand } from '@aws-sdk/client-s3';\nimport sharp from 'sharp';\nimport type { CloudFrontResponseEvent, CloudFrontResponseResult } from 'aws-lambda';\n\nconst s3 = new S3Client();\n\nexport const handler = async (event: CloudFrontResponseEvent): Promise&lt;CloudFrontResponseResult&gt; =&gt; {\n  const response = event.Records[0].cf.response;\n  const request = event.Records[0].cf.request;\n  const params = new URLSearchParams(request.querystring);\n\n  // 1. 원본 이미지 가져오기\n  const key = request.uri.substring(1);\n  const { Body } = await s3.send(new GetObjectCommand({\n    Bucket: 'my-images-bucket',\n    Key: key,\n  }));\n  const imageBuffer = Buffer.from(await Body!.transformToByteArray());\n\n  // 2. 파라미터 파싱\n  const width = parseInt(params.get('w') ?? '') || null;\n  const height = parseInt(params.get('h') ?? '') || null;\n  const format = params.get('f') ?? 'jpg';\n  const quality = parseInt(params.get('q') ?? '') || 80;\n\n  // 3. 이미지 처리\n  let image = sharp(imageBuffer);\n\n  if (width || height) {\n    image = image.resize({\n      width: width ?? undefined,\n      height: height ?? undefined,\n      fit: 'inside',           // 비율 유지\n      withoutEnlargement: true, // 원본보다 크게 안 함\n    });\n  }\n\n  // 포맷 변환\n  const formatMap = { webp: 'webp', avif: 'avif', png: 'png' } as const;\n  image = format in formatMap\n    ? image[formatMap[format as keyof typeof formatMap]]({ quality })\n    : image.jpeg({ quality });\n\n  const buffer = await image.toBuffer();\n\n  // 4. 응답 생성\n  return {\n    status: '200',\n    headers: {\n      'content-type': [{ key: 'Content-Type', value: `image/${format}` }],\n      'cache-control': [{ key: 'Cache-Control', value: 'public, max-age=31536000' }],\n      'content-length': [{ key: 'Content-Length', value: buffer.length.toString() }],\n    },\n    body: buffer.toString('base64'),\n    bodyEncoding: 'base64',\n  };\n};</code></pre><h3 id=\"lambda-메모리-최적화\">Lambda 메모리 최적화</h3>\n<p>메모리를 단계별로 올려보면서 처리 속도와 비용의 최적점을 찾았습니다.</p>\n<p><strong>메모리별 성능 비교:</strong></p>\n<ul>\n<li>512MB: 평균 5초, 타임아웃 빈번</li>\n<li>1024MB: 평균 2초, 타임아웃 거의 없음</li>\n<li>2048MB: 평균 1.8초, 성능 대비 비용 효율 낮음</li>\n</ul>\n<p><strong>비용 분석 예시:</strong></p>\n<pre><code class=\"language-\">512MB × 5초 = 2,500MB·초 (처리 느림, 타임아웃 발생)\n1024MB × 2초 = 2,048MB·초 (더 빠르면서 비용도 유사)</code></pre><p><strong>선택: 1024MB</strong></p>\n<ul>\n<li>메모리를 2배로 늘렸지만 처리 시간이 2.5배 줄어 오히려 GB·초 기준 비용이 감소</li>\n<li>타임아웃 85% 감소(20% → 3%)로 사용자 경험 대폭 개선</li>\n</ul>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"핵심-구현-3-캐시-무효화-전략\">핵심 구현 3: 캐시 무효화 전략</h2>\n<p>캐싱을 도입할 때는 항상 무효화 전략도 함께 고려해야 합니다. 상품 이미지가 교체되면 CloudFront에 캐시된 리사이즈 이미지도 무효화해야 하기 때문입니다. 두 가지 방식을 비교했습니다.</p>\n<p><strong>URL 버저닝 vs S3 이벤트 트리거:</strong></p>\n<table>\n<thead>\n<tr>\n<th>방식</th>\n<th>장점</th>\n<th>단점</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>URL 버저닝 (<code>?v=2</code>)</td>\n<td>구현 단순</td>\n<td>어드민/API 등 이미지 URL 참조하는 모든 곳 수정 필요</td>\n</tr>\n<tr>\n<td><strong>S3 이벤트 트리거</strong></td>\n<td>이미지 업로드 경로와 무관하게 중앙 처리</td>\n<td>Lambda 추가 구현 필요</td>\n</tr>\n</tbody></table>\n<p><strong>S3 이벤트 트리거 방식을 채택한 이유:</strong></p>\n<p>어드민에서 이미지를 업로드하는 경로가 상품 등록, 배너 관리, 프로모션 관리 등 다양했습니다. 각 경로마다 URL 버저닝 로직을 추가하는 것보다, S3에 이미지가 업로드되는 이벤트를 감지하여 중앙에서 처리하는 것이 개발 공수 면에서 효율적이었습니다.</p>\n<pre class=\"mermaid\">sequenceDiagram\n    participant Admin as 어드민\n    participant S3 as S3\n    participant Lambda as Lambda\n    participant CF as CloudFront\n\n    Admin->>S3: 이미지 업로드 (PutObject)\n    S3->>Lambda: S3 Event 트리거\n    Lambda->>CF: CreateInvalidation (캐시 무효화)\n    CF->>CF: 해당 이미지 캐시 삭제\n\n    Note over CF: 다음 요청 시<br/>Lambda@Edge가 재생성</pre><p>월 이미지 변경 건수가 100회 미만이므로 CloudFront Invalidation 무료 범위(월 1,000건) 내에서 운영 가능하며, 추가 비용이 발생하지 않습니다.</p>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"결과-비용-75-절감-속도-50-개선\">결과: 비용 75% 절감, 속도 50% 개선</h2>\n<p><img src=\"images/cloudfront_%EC%82%AC%EC%9A%A9%EB%9F%89_%EC%A7%80%ED%91%9C.png\" alt=\"CloudFront 사용량 지표\"></p>\n<h3 id=\"비용-절감\">비용 절감</h3>\n<p>배포는 단계적으로 진행했습니다.</p>\n<ol>\n<li><strong>5월 15일</strong>: 혜택탭 전체 적용</li>\n<li><strong>5월 20일</strong>: 상품 이미지, 배너 이미지 적용</li>\n</ol>\n<table>\n<thead>\n<tr>\n<th>시점</th>\n<th>요청 수</th>\n<th>데이터 전송량</th>\n<th>절감률</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>4월 3일</strong> (적용 전)</td>\n<td>360만 건</td>\n<td>419.71GB</td>\n<td>기준</td>\n</tr>\n<tr>\n<td><strong>5월 26일</strong> (적용 후)</td>\n<td>475만 건 (+31%)</td>\n<td>132.49GB</td>\n<td><strong>68%</strong></td>\n</tr>\n</tbody></table>\n<p>요청이 115만 건(31%) 더 많았지만, 전송량은 오히려 <strong>약 3배 감소</strong>했습니다. 이미지 최적화가 없었다면 전송량 550GB로 예상되었으나 실제 132GB로 집계되었습니다.</p>\n<p>CloudFront 데이터 전송 비용이 68% 감소했고, Lambda@Edge 실행 비용은 전체 대비 미미하여 <strong>총비용 기준 약 75% 절감</strong>으로 집계되었습니다.</p>\n<h3 id=\"성능-개선\">성능 개선</h3>\n<table>\n<thead>\n<tr>\n<th>지표</th>\n<th>Before</th>\n<th>After</th>\n<th>개선률</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>평균 이미지 크기</strong></td>\n<td>300KB</td>\n<td>80KB</td>\n<td>73%</td>\n</tr>\n<tr>\n<td><strong>페이지 로딩 속도</strong></td>\n<td>4.2초</td>\n<td>2.1초</td>\n<td>50%</td>\n</tr>\n<tr>\n<td><strong>Lambda 타임아웃</strong></td>\n<td>20%</td>\n<td>3%</td>\n<td>85% 감소</td>\n</tr>\n</tbody></table>\n<p>상품 리스트 페이지 기준, 20개 상품 이미지 총 전송량이 6MB(JPEG 원본)에서 1.2MB(WebP 리사이징)로 줄어 로딩 속도가 4.2초에서 2.1초로 <strong>50% 개선</strong>되었습니다.</p>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"트러블슈팅\">트러블슈팅</h2>\n<p><strong>1. OOM (Out Of Memory) - 대용량 이미지</strong></p>\n<table>\n<thead>\n<tr>\n<th>구분</th>\n<th>내용</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>증상</strong></td>\n<td>7952 × 5304 해상도 이미지 처리 시 Lambda OOM 발생</td>\n</tr>\n<tr>\n<td><strong>원인</strong></td>\n<td>Sharp 메모리 사용량: width × height × 4 바이트 = 약 160MB</td>\n</tr>\n<tr>\n<td><strong>해결</strong></td>\n<td>4000px 초과 이미지는 리사이징 건너뛰고 원본 반환</td>\n</tr>\n</tbody></table>\n<p>2GB 메모리로 증설해도 초대형 이미지는 OOM이 발생하므로, 현실적인 제한선을 두는 것이 필요했습니다.</p>\n<p><strong>2. Lambda 타임아웃 - 원본 응답 불가</strong></p>\n<table>\n<thead>\n<tr>\n<th>구분</th>\n<th>내용</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>증상</strong></td>\n<td>처리 시간이 30초를 초과하면 응답 자체가 없음</td>\n</tr>\n<tr>\n<td><strong>원인</strong></td>\n<td>Lambda 자체가 종료되어 catch 블록도 실행 안 됨 → 원본 반환도 불가</td>\n</tr>\n<tr>\n<td><strong>해결</strong></td>\n<td><code>Promise.race</code>로 28초 타임아웃을 걸어 시간 초과 시 원본 이미지 반환</td>\n</tr>\n</tbody></table>\n<p>CloudFront Lambda@Edge는 30초 제한이므로, 2초 여유를 두고 타임아웃을 설정했습니다.</p>\n<p><strong>3. 확장자 없는 이미지 파일</strong></p>\n<table>\n<thead>\n<tr>\n<th>구분</th>\n<th>내용</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>증상</strong></td>\n<td><code>product-image</code> 같은 확장자 없는 파일이 리사이징 대상에서 제외됨</td>\n</tr>\n<tr>\n<td><strong>원인</strong></td>\n<td>파일명의 확장자로 이미지 여부를 판단하는 로직</td>\n</tr>\n<tr>\n<td><strong>해결</strong></td>\n<td>S3 응답의 <code>Content-Type</code> 헤더로 이미지 여부 판단으로 변경</td>\n</tr>\n</tbody></table>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"기술-스택\">기술 스택</h2>\n<table>\n<thead>\n<tr>\n<th>분류</th>\n<th>기술</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>CDN</strong></td>\n<td>CloudFront</td>\n</tr>\n<tr>\n<td><strong>스토리지</strong></td>\n<td>S3</td>\n</tr>\n<tr>\n<td><strong>이미지 처리</strong></td>\n<td>Lambda@Edge (Node.js 18, Sharp)</td>\n</tr>\n<tr>\n<td><strong>요청 검증</strong></td>\n<td>CloudFront Functions</td>\n</tr>\n</tbody></table>\n</section>\n"
  },
  {
    "id": "opensearch-search-engine",
    "title": "검색 엔진 전면 개편",
    "tags": [
      "AWS OpenSearch",
      "NestJS",
      "Redis",
      "Lambda",
      "Redshift"
    ],
    "content": "<section class=\"content-section\">\n<h2 id=\"목차\">목차</h2>\n<ol>\n<li><a href=\"#%EB%B0%B0%EA%B2%BD-%EB%8A%90%EB%A6%B0-%EA%B2%80%EC%83%89-%EB%B6%88%EC%95%88%EC%A0%95%ED%95%9C-%EC%84%9C%EB%B2%84\">배경: 느린 검색, 불안정한 서버</a></li>\n<li><a href=\"#%EB%AC%B8%EC%A0%9C-%EB%B6%84%EC%84%9D-fusejs%EC%9D%98-%ED%95%9C%EA%B3%84\">문제 분석: Fuse.js의 한계</a></li>\n<li><a href=\"#%ED%95%B4%EA%B2%B0-%EB%AA%A9%ED%91%9C-%EB%AC%B4%EC%97%87%EC%9D%84-%EB%8B%AC%EC%84%B1%ED%95%98%EB%A0%A4-%ED%96%88%EB%82%98\">해결 목표: 무엇을 달성하려 했나</a></li>\n<li><a href=\"#%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98-%EC%84%A4%EA%B3%84-opensearch%EB%A5%BC-%EC%84%A0%ED%83%9D%ED%95%9C-%EC%9D%B4%EC%9C%A0\">아키텍처 설계: OpenSearch를 선택한 이유</a></li>\n<li><a href=\"#opensearch-%EC%A0%84%ED%99%98-%EA%B2%B0%EA%B3%BC\">OpenSearch 전환 결과</a></li>\n<li><a href=\"#%ED%95%B5%EC%8B%AC-%EA%B5%AC%ED%98%84-1-ctr-%EA%B8%B0%EB%B0%98-%EB%8F%99%EC%A0%81-%EB%9E%AD%ED%82%B9\">핵심 구현 1: CTR 기반 동적 랭킹</a></li>\n<li><a href=\"#%ED%95%B5%EC%8B%AC-%EA%B5%AC%ED%98%84-2-%EC%BA%90%EC%8B%9C-%EC%A0%84%EB%9E%B5%EC%9C%BC%EB%A1%9C-%EC%9D%91%EB%8B%B5-%EC%86%8D%EB%8F%84-%EA%B0%9C%EC%84%A0\">핵심 구현 2: 캐시 전략으로 응답 속도 개선</a></li>\n<li><a href=\"#%EA%B2%B0%EA%B3%BC-%EC%88%AB%EC%9E%90%EB%A1%9C-%EC%A6%9D%EB%AA%85%ED%95%98%EB%8A%94-%EA%B0%9C%EC%84%A0-%ED%9A%A8%EA%B3%BC\">결과: 숫자로 증명하는 개선 효과</a></li>\n</ol>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"배경-느린-검색-불안정한-서버\">배경: 느린 검색, 불안정한 서버</h2>\n<p>300만 회원이 사용하는 커머스 플랫폼에서 검색은 핵심 기능입니다. 하지만 &quot;나이키 운동화&quot;를 검색하면 결과가 나오기까지 1~2초가 걸렸고, 키워드가 길어지면 훨씬 더 오래 걸렸습니다. 검색 후 상품을 클릭하는 비율(CTR)은 17%에 불과했습니다.</p>\n<p>더 심각한 문제는 서버 안정성이었습니다. 검색을 위해 서버 메모리에 전체 상품 데이터를 올려놓는 구조였기 때문에, 상품 수가 증가할수록 서버 메모리와 CPU에 부담이 가중되었습니다. 검색 순위를 조정하려면 코드를 수정하고 프로덕션에 배포해야 했고, 마케팅팀의 요청 하나에도 30분이 소요되었습니다.</p>\n<p>검색 엔진을 전면 개편하기로 결정했습니다.</p>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"문제-분석-fusejs의-한계\">문제 분석: Fuse.js의 한계</h2>\n<p>기존 시스템은 <strong>Fuse.js</strong>라는 JavaScript 기반 퍼지 검색 라이브러리를 사용했습니다. Fuse.js는 역 인덱스(inverted index) 없이 전체 데이터를 순회하며 문자열 유사도를 계산하는 방식으로 동작합니다. 이를 서버 메모리에 전체 상품 데이터를 로드한 후 검색을 수행하는 방식으로 사용하고 있었습니다.</p>\n<h3 id=\"왜-문제였을까\">왜 문제였을까?</h3>\n<ol>\n<li><p><strong>서버 자원 과다 사용</strong></p>\n<ul>\n<li>전체 상품 데이터(수만 건)를 서버 메모리에 상주시켜야 함</li>\n<li>검색할 때마다 전체 데이터를 순회하며 유사도 계산 → CPU 스파이크 발생</li>\n<li>검색 외 다른 API 응답 속도까지 저하되는 영향</li>\n</ul>\n</li>\n<li><p><strong>상품 수 증가에 취약한 구조</strong></p>\n<ul>\n<li>역 인덱스가 없어 상품이 늘어날수록 검색 시간이 선형으로 증가</li>\n<li>&quot;나이키 에어맥스 270 블랙 런닝화&quot; 같은 긴 검색어는 특히 느림</li>\n<li>서버 메모리 부족으로 재시작이 빈번</li>\n</ul>\n</li>\n<li><p><strong>비즈니스 민첩성 부족</strong></p>\n<ul>\n<li>검색 순위 조정 = 코드 수정 + 배포 (30분)</li>\n<li>A/B 테스트 불가능</li>\n<li>동의어 추가도 배포 필요</li>\n</ul>\n</li>\n<li><p><strong>데이터 품질 오염</strong></p>\n<ul>\n<li>검색 정확도를 높이기 위해 상품명에 검색용 키워드를 강제로 부착</li>\n<li>예: &quot;나이키 신발&quot; → &quot;나이키 신발 운동화 스니커즈 런닝화&quot;</li>\n<li>사용자에게 보이는 상품명도 지저분해짐</li>\n</ul>\n</li>\n</ol>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"해결-목표-무엇을-달성하려-했나\">해결 목표: 무엇을 달성하려 했나</h2>\n<h3 id=\"정량적-목표\">정량적 목표</h3>\n<ul>\n<li><strong>응답 속도</strong>: 1~2초 → 핵심 검색 엔드포인트 p95 200ms 이하</li>\n<li><strong>CTR</strong>: 17% → 20% 이상 (30% 향상)</li>\n</ul>\n<h3 id=\"정성적-목표\">정성적 목표</h3>\n<ul>\n<li>상품 수가 증가해도 대응 가능한 확장 가능 구조</li>\n<li>배포 없이 검색 알고리즘을 조정할 수 있는 유연한 방식</li>\n<li>서버 메모리 부담 감소로 안정성 확보</li>\n</ul>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"아키텍처-설계-opensearch를-선택한-이유\">아키텍처 설계: OpenSearch를 선택한 이유</h2>\n<h3 id=\"opensearch-vs-cloudsearch\">OpenSearch vs CloudSearch</h3>\n<p>AWS에는 두 가지 관리형 검색 서비스가 있습니다. 저희는 AWS 생태계 내에서 인프라를 운영 중이었기 때문에, 검색 서비스인 Amazon CloudSearch와 Amazon OpenSearch Service 중 하나를 선택해야 했습니다.</p>\n<table>\n<thead>\n<tr>\n<th>기준</th>\n<th>CloudSearch</th>\n<th>OpenSearch</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>기반 엔진</strong></td>\n<td>Apache Solr</td>\n<td>Elasticsearch 포크 (OpenSearch)</td>\n</tr>\n<tr>\n<td><strong>커스텀 스코어링</strong></td>\n<td>제한적 (표현식 기반)</td>\n<td>Function Score Query로 자유로운 구현</td>\n</tr>\n<tr>\n<td><strong>한국어 분석</strong></td>\n<td>기본 토크나이저</td>\n<td>Nori 형태소 분석기</td>\n</tr>\n<tr>\n<td><strong>플러그인 확장</strong></td>\n<td>미지원</td>\n<td>커스텀 플러그인 지원</td>\n</tr>\n<tr>\n<td><strong>스케일링</strong></td>\n<td>자동</td>\n<td>인스턴스/샤드 직접 제어</td>\n</tr>\n</tbody></table>\n<blockquote>\n<p><strong>참고</strong>: Amazon CloudSearch는 현재 신규 고객에게 더 이상 제공되지 않으며, AWS는 OpenSearch Service로의 전환을 권장하고 있습니다.</p>\n</blockquote>\n<p><strong>OpenSearch를 선택한 이유:</strong></p>\n<ul>\n<li>CTR 기반 커스텀 스코어링(Function Score Query) 구현 가능</li>\n<li>Nori 형태소 분석기로 한국어 검색 품질 확보</li>\n<li>기존 AWS 인프라(Redshift, Lambda, S3)와 자연스러운 통합</li>\n<li>동의어/사용자 사전을 플러그인으로 관리 가능</li>\n</ul>\n<h3 id=\"전체-아키텍처\">전체 아키텍처</h3>\n<p><img src=\"images/%EA%B2%80%EC%83%89%20%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98.png\" alt=\"검색 아키텍처\"></p>\n<p><strong>핵심 컴포넌트:</strong></p>\n<ol>\n<li><strong>NestJS API (ECS)</strong>: 검색 요청 처리 및 비즈니스 로직</li>\n<li><strong>Redis (ElastiCache)</strong>: 메타데이터(상품 수, 필터), 즉시할인 비용, 상단 노출 리스트 캐싱</li>\n<li><strong>OpenSearch 클러스터</strong>: Master Node 3대 + Data Node 2대로 전문 검색 및 CTR 기반 랭킹</li>\n</ol>\n<p><strong>아키텍처 구성 이유:</strong></p>\n<ul>\n<li><strong>ECS 검색 컨테이너 2대 + ALB</strong>: 검색 서비스를 별도 컨테이너로 분리하여, 검색 트래픽 급증 시에도 메인 API 서버에 영향을 주지 않도록 격리했습니다. ALB를 통해 두 컨테이너에 트래픽을 분산하여 가용성을 확보합니다.</li>\n<li><strong>Master Node 3대 (전용)</strong>: AWS가 프로덕션 환경에서 권장하는 구성으로, 전용 Master Node를 홀수(3대)로 배치하여 Split-Brain 방지를 위한 과반수 투표(quorum)를 보장합니다. Master와 Data 역할을 분리함으로써, 무거운 검색/색인 작업이 클러스터 안정성에 영향을 주지 않습니다.</li>\n<li><strong>Data Node 2대</strong>: 실제 데이터 저장과 검색 쿼리를 처리합니다. Replica 1로 설정하여 한 노드에 장애가 발생해도 검색이 중단되지 않습니다.</li>\n<li><strong>ElastiCache Redis</strong>: OpenSearch 앞단에 캐시 레이어를 두어, 메타데이터 등 반복 조회를 흡수하고 OpenSearch 부하를 줄입니다.</li>\n</ul>\n<h3 id=\"인덱스-설계\">인덱스 설계</h3>\n<p>인덱스 설계의 핵심 원칙은 두 가지였습니다. 첫째, <strong>상품 검색 키워드와 사전(동의어/사용자 사전)을 통해 검색 품질을 최대화</strong>하는 것. 둘째, <strong>검색 점수에 영향을 주는 요소들을 운영팀이 직접 관리</strong>할 수 있도록 하는 것. 이를 위해 불필요한 복잡성은 최대한 덜어내고, 분석기와 매핑을 다음과 같이 설계했습니다.</p>\n<p><strong>한국어 분석기 구성:</strong></p>\n<pre><code class=\"language-json\">{\n  &quot;settings&quot;: {\n    &quot;analysis&quot;: {\n      &quot;char_filter&quot;: {\n        &quot;ascii_lowercase_filter&quot;: {\n          &quot;type&quot;: &quot;mapping&quot;,\n          &quot;mappings&quot;: [&quot;A =&gt; a&quot;, &quot;B =&gt; b&quot;, &quot;...&quot;]\n        }\n      },\n      &quot;tokenizer&quot;: {\n        &quot;mixed_nori_tokenizer&quot;: {\n          &quot;type&quot;: &quot;nori_tokenizer&quot;,\n          &quot;decompound_mode&quot;: &quot;mixed&quot;,\n          &quot;discard_punctuation&quot;: true,\n          &quot;user_dictionary&quot;: &quot;user_dictionary.txt&quot;\n        }\n      },\n      &quot;filter&quot;: {\n        &quot;synonym_filter&quot;: {\n          &quot;type&quot;: &quot;synonym_graph&quot;,\n          &quot;synonyms_path&quot;: &quot;synonyms.txt&quot;,\n          &quot;updateable&quot;: true\n        }\n      },\n      &quot;analyzer&quot;: {\n        &quot;index_analyzer&quot;: {\n          &quot;char_filter&quot;: [&quot;ascii_lowercase_filter&quot;],\n          &quot;tokenizer&quot;: &quot;mixed_nori_tokenizer&quot;,\n          &quot;filter&quot;: [&quot;lowercase&quot;, &quot;trim&quot;]\n        },\n        &quot;search_analyzer&quot;: {\n          &quot;char_filter&quot;: [&quot;ascii_lowercase_filter&quot;],\n          &quot;tokenizer&quot;: &quot;mixed_nori_tokenizer&quot;,\n          &quot;filter&quot;: [&quot;lowercase&quot;, &quot;synonym_filter&quot;, &quot;trim&quot;]\n        }\n      }\n    }\n  }\n}</code></pre><table>\n<thead>\n<tr>\n<th>설정</th>\n<th>값</th>\n<th>이유</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>decompound_mode</code></td>\n<td><code>mixed</code></td>\n<td>&quot;삼성전자&quot; → [&quot;삼성전자&quot;, &quot;삼성&quot;, &quot;전자&quot;] 원본 + 분해 토큰을 모두 생성하여 복합어/단독 검색 모두 대응</td>\n</tr>\n<tr>\n<td><code>user_dictionary</code></td>\n<td>사용자 사전 파일</td>\n<td>Nori 기본 사전에 없는 브랜드명, 신조어 등을 등록하여 형태소 분석 정확도 향상</td>\n</tr>\n<tr>\n<td><code>char_filter</code></td>\n<td>ASCII 소문자 변환</td>\n<td>Nori 토크나이저 진입 전에 대소문자를 통일하여, &quot;Nike&quot;와 &quot;nike&quot;가 동일 토큰으로 처리</td>\n</tr>\n<tr>\n<td><code>synonym_graph</code></td>\n<td><code>updateable: true</code></td>\n<td>검색 분석기에만 적용하여 재색인 없이 동의어 업데이트 가능</td>\n</tr>\n<tr>\n<td>분석기 분리</td>\n<td>index / search</td>\n<td>색인 시에는 원본 그대로, 검색 시에만 동의어를 확장하여 인덱스 크기 증가 없이 검색 품질 확보</td>\n</tr>\n</tbody></table>\n<p><strong>매핑 최적화:</strong></p>\n<pre><code class=\"language-json\">{\n  &quot;mappings&quot;: {\n    &quot;properties&quot;: {\n      &quot;front_name&quot;: {\n        &quot;type&quot;: &quot;text&quot;,\n        &quot;fields&quot;: {\n          &quot;keyword&quot;: { &quot;type&quot;: &quot;keyword&quot; },\n          &quot;nori&quot;: {\n            &quot;type&quot;: &quot;text&quot;,\n            &quot;analyzer&quot;: &quot;index_analyzer&quot;,\n            &quot;search_analyzer&quot;: &quot;search_analyzer&quot;\n          }\n        }\n      },\n      &quot;tag_groups&quot;: { &quot;type&quot;: &quot;keyword&quot;, &quot;eager_global_ordinals&quot;: true },\n      &quot;tag_names&quot;: { &quot;type&quot;: &quot;keyword&quot;, &quot;eager_global_ordinals&quot;: true },\n      &quot;category_mains&quot;: {\n        &quot;type&quot;: &quot;nested&quot;,\n        &quot;properties&quot;: {\n          &quot;name&quot;: {\n            &quot;type&quot;: &quot;text&quot;,\n            &quot;analyzer&quot;: &quot;index_analyzer&quot;,\n            &quot;copy_to&quot;: &quot;combined_category_name&quot;\n          }\n        }\n      },\n      &quot;thumbnail_url&quot;: { &quot;type&quot;: &quot;keyword&quot;, &quot;index&quot;: false },\n      &quot;description&quot;: { &quot;type&quot;: &quot;text&quot;, &quot;index&quot;: false }\n    }\n  }\n}</code></pre><table>\n<thead>\n<tr>\n<th>설정</th>\n<th>적용 필드</th>\n<th>이유</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>eager_global_ordinals</code></td>\n<td><code>tag_groups</code>, <code>tag_names</code></td>\n<td>태그 집계(terms aggregation) 시 Global Ordinals를 미리 로드하여 첫 쿼리 지연 제거</td>\n</tr>\n<tr>\n<td><code>copy_to</code></td>\n<td>카테고리(main/sub/third) → <code>combined_category_name</code></td>\n<td>3단계 카테고리 이름을 하나의 필드로 합쳐서 단일 쿼리로 전체 카테고리 검색</td>\n</tr>\n<tr>\n<td><code>index: false</code></td>\n<td><code>thumbnail_url</code>, <code>description</code> 등</td>\n<td>표시 전용 필드는 역 인덱스를 생성하지 않아 디스크/메모리 절약</td>\n</tr>\n</tbody></table>\n<h3 id=\"색인-및-사전-업데이트-파이프라인\">색인 및 사전 업데이트 파이프라인</h3>\n<p>검색 아키텍처와 별도로, 상품 데이터를 OpenSearch에 동기화하고 검색 품질을 관리하는 파이프라인을 구성했습니다.</p>\n<p><img src=\"images/%EC%83%89%EC%9D%B8_%EC%82%AC%EC%A0%84_%EC%97%85%EB%8D%B0%EC%9D%B4%ED%8A%B8_%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98.png\" alt=\"색인 및 사전 업데이트 아키텍처\"></p>\n<ul>\n<li><strong>상품 전체 색인 (Lambda)</strong>: EventBridge 스케줄러가 1시간 주기로 Lambda를 트리거하여, DB에서 상품 데이터를 읽고 OpenSearch에 전체 색인을 수행합니다. 이때 Athena에서 집계한 CTR 데이터도 함께 색인됩니다.</li>\n<li><strong>동의어 업데이트 (Lambda)</strong>: 운영팀이 S3에 업로드한 동의어 파일을 Lambda가 감지하여 OpenSearch 패키지를 자동으로 업데이트합니다. <code>synonym_graph</code>의 <code>updateable: true</code> 설정 덕분에 재색인 없이 반영됩니다.</li>\n<li><strong>사용자 사전 업데이트 (Lambda)</strong>: 사용자 사전은 토크나이저에서 사용되므로 패키지 업데이트 후 재색인이 필요합니다. 1시간 주기 전체 색인 시 자동으로 반영되기 때문에 별도 작업 없이 다음 색인 사이클에서 적용됩니다.</li>\n</ul>\n<p><strong>상품 전체 색인 플로우:</strong></p>\n<p>색인 Lambda는 단순히 DB를 복사하는 것이 아니라, 여러 소스의 데이터를 조합하여 검색에 최적화된 문서를 생성합니다.</p>\n<pre><code class=\"language-\">1. 타임스탬프 기반 새 인덱스 생성 (예: items_2024-06-01-14-00-00)\n2. Athena에서 7일 롤링 CTR 집계\n3. DB에서 커서 기반으로 1,000건씩 조회하며:\n   - 상품 기본 정보 + 브랜드/카테고리 메타 (DB JOIN)\n   - 태그, 리뷰, 추천 점수 (DB)\n   - CTR (Athena 집계 결과)\n   - 상품코드 정규화 (code_normalized)\n   → 조합하여 Bulk 색인\n4. Alias를 새 인덱스로 원자적 전환\n5. 1일 이전 구 인덱스 삭제</code></pre><p>핵심은 <strong>Alias 전환이 원자적(atomic)</strong>이라는 점입니다. 기존 인덱스 제거와 새 인덱스 추가를 단일 API 호출로 처리하므로, 전환 과정에서 검색이 실패하는 순간이 없습니다.</p>\n<pre><code class=\"language-typescript\">// Alias 전환 - 단일 API 호출로 원자적 처리\nawait opensearchClient.indices.updateAliases({\n  body: {\n    actions: [\n      ...oldIndexNames.map(name =&gt; ({\n        remove: { index: name, alias: ALIAS }\n      })),\n      { add: { index: newIndexName, alias: ALIAS } }\n    ]\n  }\n});</code></pre><p><strong>커서 기반 Bulk 색인:</strong></p>\n<p>수만 건의 상품을 한 번에 메모리에 올리면 Lambda의 메모리 한계에 도달할 수 있습니다. <code>lastId</code> 기반 커서 페이지네이션으로 1,000건씩 처리하여 메모리 사용량을 일정하게 유지합니다.</p>\n<pre><code class=\"language-typescript\">const CHUNK_SIZE = 1000;\nlet lastId: number = 0;\n\nconst searchCTRMap = await getSearchCTRFromAthena();\n\nwhile (true) {\n  const [items] = await getItemInfo(dbConnection, CHUNK_SIZE, lastId);\n  if (items.length === 0) break;\n  lastId = items[items.length - 1].id;\n\n  // 태그, 리뷰, 추천 점수를 병렬 조회\n  const [tags, reviews, recommendRatings] = await Promise.all([\n    getItemTags(items, dbConnection),\n    getReviews(items, dbConnection),\n    getRecommendRatings(items, dbConnection)\n  ]);\n\n  const bulkOps = items.flatMap(item =&gt; [\n    { index: { _index: newIndexName, _id: item.id } },\n    {\n      ...item,\n      recommend_rating: recommendRatings.get(item.id) ?? 0,\n      review_count: reviews.get(item.id)?.reviewCount ?? 0,\n    }\n  ]);\n  await opensearchClient.bulk({ refresh: true, body: bulkOps });\n}</code></pre><hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"opensearch-전환-결과\">OpenSearch 전환 결과</h2>\n<p>Fuse.js에서 OpenSearch로 전환한 것만으로도 핵심 문제들이 해소되었습니다.</p>\n<table>\n<thead>\n<tr>\n<th>지표</th>\n<th>Before (Fuse.js)</th>\n<th>After (OpenSearch)</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>검색 응답 속도</strong></td>\n<td>1~2초</td>\n<td>300ms</td>\n</tr>\n<tr>\n<td><strong>서버 메모리</strong></td>\n<td>전체 상품 데이터 상주 (수백 MB)</td>\n<td>검색 전용 클러스터로 분리</td>\n</tr>\n<tr>\n<td><strong>상품 수 증가 대응</strong></td>\n<td>선형으로 느려짐</td>\n<td>역 인덱스 기반으로 일정한 성능 유지</td>\n</tr>\n</tbody></table>\n<p>서버 메모리에 상품 데이터를 올려놓고 전체 순회하던 구조에서, 검색을 OpenSearch 클러스터로 완전히 분리함으로써 <strong>서버 안정성이 확보</strong>되었고, <strong>검색 응답 속도가 약 5배 개선</strong>되었습니다.</p>\n<p>하지만 여기서 멈추지 않았습니다. 300ms는 충분히 빠르지만, 검색 품질(CTR 17%)에는 여전히 개선 여지가 있었습니다. 이를 해결하기 위해 두 가지 핵심 고도화를 진행했습니다.</p>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"핵심-구현-1-ctr-기반-동적-랭킹\">핵심 구현 1: CTR 기반 동적 랭킹</h2>\n<h3 id=\"문제-검색-결과를-어떻게-정렬할-것인가\">문제: 검색 결과를 어떻게 정렬할 것인가?</h3>\n<p>OpenSearch 전환 후 기본 BM25 스코어만 사용했습니다. 하지만 이는 텍스트 유사도만 고려할 뿐, 실제로 사용자가 선호하는 상품을 반영하지 못했습니다.</p>\n<p>예를 들어, &quot;운동화&quot; 검색 시:</p>\n<ul>\n<li>상품 A: 상품명에 &quot;운동화&quot;가 3번 등장 → 높은 점수</li>\n<li>상품 B: 상품명에 &quot;운동화&quot;가 1번만 등장 → 낮은 점수</li>\n</ul>\n<p>하지만 <strong>상품 B의 CTR이 25%</strong>, <strong>상품 A의 CTR은 5%</strong>라면 어떨까요? 사용자는 명백히 상품 B를 선호하는데, 검색 결과는 상품 A를 상위에 노출하고 있었습니다.</p>\n<h3 id=\"해결-색인-시-ctr-데이터를-함께-저장\">해결: 색인 시 CTR 데이터를 함께 저장</h3>\n<p><strong>CTR(Click-Through Rate, 클릭률)</strong>을 검색 랭킹에 반영하기 위해, 상품 데이터를 색인할 때 CTR 정보를 함께 저장하는 방식을 채택했습니다.</p>\n<p><strong>1단계: Athena에서 7일 롤링 CTR 집계</strong></p>\n<p>매 색인 시, Athena(Presto SQL)에서 최근 7일간의 사용자 행동 로그를 분석하여 상품별 CTR을 집계합니다. 검색 결과 노출(<code>view_search_result</code>)과 상품 클릭(<code>view_item</code>) 이벤트를 별도로 집계한 뒤 <code>FULL OUTER JOIN</code>으로 결합합니다.</p>\n<pre><code class=\"language-sql\">-- Presto/Athena SQL\nWITH search_views AS (\n    SELECT item.item_id, COUNT(*) as view_count\n    FROM user_action_log\n    CROSS JOIN UNNEST(\n        CAST(json_parse(items) AS ARRAY(ROW(item_id INTEGER)))\n    ) AS t(item)\n    WHERE event_type = 'view_search_result'\n        AND dt BETWEEN DATE '${oneWeekAgo}' AND DATE '${today}'\n    GROUP BY item.item_id\n),\nsearch_clicks AS (\n    SELECT item_id, COUNT(*) as click_count\n    FROM user_action_log\n    WHERE event_type = 'view_item'\n        AND item_list_name LIKE '/search%'\n        AND dt BETWEEN DATE '${oneWeekAgo}' AND DATE '${today}'\n    GROUP BY item_id\n)\nSELECT\n    COALESCE(sv.item_id, sc.item_id) as item_id,\n    CASE\n        WHEN COALESCE(sv.view_count, 0) &gt;= 100\n        THEN CAST(sc.click_count AS DOUBLE) / CAST(sv.view_count AS DOUBLE) * 100\n        ELSE NULL\n    END as ctr\nFROM search_views sv\nFULL OUTER JOIN search_clicks sc ON sv.item_id = sc.item_id\nWHERE COALESCE(sv.view_count, 0) &gt;= 100  -- 최소 노출 100회 이상</code></pre><p><code>CROSS JOIN UNNEST</code>를 사용하는 이유는, 검색 결과 노출 이벤트에 노출된 상품 목록이 JSON 배열로 저장되어 있기 때문입니다. 이를 풀어서 상품별 노출 횟수를 집계합니다.</p>\n<p><strong>2단계: 색인 시 CTR을 상품 문서에 포함</strong></p>\n<p>1단계에서 집계한 CTR 데이터는 색인 파이프라인에서 상품 정보와 함께 OpenSearch에 저장됩니다. 이렇게 하면 검색 시 별도의 CTR 조회 없이, Function Score Query로 CTR을 랭킹에 바로 반영할 수 있습니다.</p>\n<p><strong>신규 상품의 Cold-start 처리:</strong></p>\n<p>CTR 기반 랭킹의 가장 큰 약점은 신규 상품입니다. 노출 데이터가 없는 상품은 CTR이 0이므로, 검색 결과에서 밀려나 노출 기회를 얻지 못하는 악순환이 발생합니다(Exploration vs Exploitation 문제).</p>\n<p>이를 해결하기 위해 <strong>최소 노출 기준(100회) 미달 상품에는 기본 CTR 값을 부여</strong>했습니다. 기본 CTR은 해당 카테고리 상품들의 평균 CTR로 설정하여, 검색 점수에 중립적 영향(증가도 감소도 아닌)을 미치도록 했습니다. 신규 상품이 자연스럽게 노출되어 실제 CTR 데이터가 쌓이면 기본값은 실측값으로 교체됩니다.</p>\n<pre><code class=\"language-sql\">-- 신규 상품에 카테고리 평균 CTR 부여\nWITH category_avg AS (\n  SELECT category_id, AVG(ctr) AS avg_ctr\n  FROM product_ctr_stats\n  WHERE impressions &gt;= 100\n  GROUP BY category_id\n)\nSELECT\n  p.product_id,\n  COALESCE(s.ctr, ca.avg_ctr, 5.0) AS ctr  -- 실측값(%) → 카테고리 평균(%) → 기본값 5%\nFROM products p\nLEFT JOIN product_ctr_stats s ON p.product_id = s.product_id AND s.impressions &gt;= 100\nLEFT JOIN category_avg ca ON p.category_id = ca.category_id;</code></pre><p><strong>3단계: Function Score Query + Painless Script로 BM25 + CTR 결합</strong></p>\n<p>실제 운영에서는 OpenSearch의 <code>function_score</code> 쿼리 안에 Painless 스크립트를 넣어 CTR을 반영했습니다. CTR은 퍼센트 단위(예: 22.5)로 저장하고, 최종 점수는 <code>BM25 * log10(10 + CTR%)</code>로 계산했습니다.</p>\n<pre><code class=\"language-json\">{\n  &quot;query&quot;: {\n    &quot;function_score&quot;: {\n      &quot;query&quot;: {\n        &quot;bool&quot;: {\n          &quot;must&quot;: [\n            {\n              &quot;multi_match&quot;: {\n                &quot;query&quot;: &quot;나이키 운동화&quot;,\n                &quot;fields&quot;: [&quot;product_name^3&quot;, &quot;brand^2&quot;, &quot;category&quot;]\n              }\n            }\n          ]\n        }\n      },\n      &quot;functions&quot;: [\n        {\n          &quot;script_score&quot;: {\n            &quot;script&quot;: {\n              &quot;lang&quot;: &quot;painless&quot;,\n              &quot;source&quot;: &quot;double ctr = doc['search_ctr'].size() == 0 ? params.default_ctr : doc['search_ctr'].value; return Math.log10(params.offset + ctr);&quot;,\n              &quot;params&quot;: {\n                &quot;offset&quot;: 10.0,\n                &quot;default_ctr&quot;: 1.0\n              }\n            }\n          }\n        }\n      ],\n      &quot;boost_mode&quot;: &quot;multiply&quot;,\n      &quot;score_mode&quot;: &quot;sum&quot;\n    }\n  }\n}</code></pre><ul>\n<li><code>Math.log10(10 + CTR%)</code> — CTR=0이어도 <code>log10(10)=1</code>로 안전하게 계산되고, 극단값 영향도 완화</li>\n<li><code>default_ctr: 1.0</code> — <code>search_ctr</code>가 없는 문서(신규/누락 데이터)는 기본 CTR 1% 사용</li>\n<li><code>boost_mode: &quot;multiply&quot;</code> — BM25 스코어에 스크립트 점수를 곱해 최종 점수 산출</li>\n<li>최종 점수: <code>BM25 * log10(10 + CTR%)</code></li>\n</ul>\n<h3 id=\"검증-ab-테스트\">검증: A/B 테스트</h3>\n<p>1주일간 세션 기반 A/B 테스트를 진행했습니다.</p>\n<ul>\n<li><strong>그룹 A (기존)</strong>: Fuse.js 기반 기존 검색 시스템</li>\n<li><strong>그룹 B (개선)</strong>: OpenSearch + CTR 기반 랭킹 시스템</li>\n</ul>\n<p><strong>결과:</strong></p>\n<ul>\n<li>CTR: 17% → 22.5% (<strong>+5.5%p, 상대 32% 향상</strong>)</li>\n<li>상품 상세 페이지 진입률: (<strong>11.75% 증가</strong>) (A/B Test, p &lt; 0.01)</li>\n<li>측정 기준: 세션 기반 A/B 테스트, 1주 관측</li>\n</ul>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"핵심-구현-2-캐시-전략으로-응답-속도-개선\">핵심 구현 2: 캐시 전략으로 응답 속도 개선</h2>\n<h3 id=\"문제-검색-과정에서-부가-조회가-많다\">문제: 검색 과정에서 부가 조회가 많다</h3>\n<p>검색은 단순히 OpenSearch에 쿼리 한 번으로 끝나지 않습니다. 실제 검색 요청은 두 단계로 이루어집니다.</p>\n<p><strong>1단계: 메타데이터 조회 → HTML 렌더링</strong></p>\n<ul>\n<li>검색어에 해당하는 전체 문서 수, 태그 필터 옵션을 OpenSearch에서 조회</li>\n<li>이 데이터로 검색 결과 페이지 HTML을 렌더링하여 클라이언트에 전달</li>\n</ul>\n<p><strong>2단계: 상품 검색 → 가격/할인 반영 → 최종 노출</strong></p>\n<ul>\n<li>클라이언트에서 검색어에 매칭되는 상품을 OpenSearch에서 조회</li>\n<li>조회된 상품에 대해 DB에서 최신 가격/재고 데이터를 갱신</li>\n<li>즉시할인 데이터를 전체 조회하여 각 상품에 조건 매칭 후 할인 반영</li>\n<li>최종 검색 결과 노출</li>\n</ul>\n<p>OpenSearch 전환으로 검색 자체는 300ms로 개선되었지만, 메타데이터 조회와 즉시할인 계산 등 부가 작업까지 합치면 추가 개선 여지가 있었습니다.</p>\n<pre class=\"mermaid\">sequenceDiagram\n    participant C as 클라이언트\n    participant A as API Server\n    participant R as Redis\n    participant O as OpenSearch\n    participant D as DB\n\n    Note over C,A: 1단계: 메타데이터 조회\n    C->>A: 검색 요청 \"나이키 운동화\"\n    rect rgb(50, 120, 50)\n        Note over A,R: ✅ 캐싱 적용\n        A->>R: 메타데이터 (전체 문서 수, 태그 필터)\n        R-->>A: 캐시 HIT\n    end\n    A-->>C: 검색 페이지 HTML 렌더링\n\n    Note over C,A: 2단계: 상품 검색\n    C->>A: 상품 목록 요청\n    A->>O: Function Score Query (BM25 + CTR)\n    O-->>A: 상품 검색 결과\n    A->>D: 가격/재고 최신 데이터 조회\n    D-->>A: 가격/재고 반영\n    rect rgb(50, 120, 50)\n        Note over A,R: ✅ 캐싱 적용\n        A->>R: 즉시할인 데이터 전체 조회\n        R-->>A: 캐시 HIT\n    end\n    A->>A: 상품별 할인 조건 매칭 및 반영\n    A-->>C: 최종 검색 결과 노출</pre><h3 id=\"해결-redis-캐싱-1분-ttl\">해결: Redis 캐싱 (1분 TTL)</h3>\n<p>자주 변하지 않는 부가 데이터를 Redis에 1분 주기로 캐싱했습니다.</p>\n<p><strong>캐싱 대상:</strong></p>\n<table>\n<thead>\n<tr>\n<th>캐싱 항목</th>\n<th>단계</th>\n<th>설명</th>\n<th>TTL</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>메타데이터</td>\n<td>1단계</td>\n<td>전체 문서 수, 태그 필터 옵션</td>\n<td>1분</td>\n</tr>\n<tr>\n<td>즉시할인 데이터</td>\n<td>2단계</td>\n<td>전체 즉시할인 목록 (조건 매칭에 사용)</td>\n<td>1분</td>\n</tr>\n</tbody></table>\n<pre><code class=\"language-typescript\">async getSearchMetadata(keyword: string): Promise&lt;SearchMetadata&gt; {\n  const cacheKey = `search:meta:${keyword}`;\n\n  const cached = await this.redis.get(cacheKey);\n  if (cached) {\n    return JSON.parse(cached);\n  }\n\n  const metadata = await this.openSearch.getMetadata(keyword);\n  await this.redis.setex(cacheKey, 60, JSON.stringify(metadata));  // TTL 60초\n\n  return metadata;\n}</code></pre><p>캐시 키는 <strong>검색어 기반</strong>으로 구성합니다. 동일한 검색어에 대한 메타데이터(문서 수, 태그 필터)는 1분간 동일하므로, 같은 검색어 요청이 반복될 때 OpenSearch 조회를 생략합니다.</p>\n<p>1분 TTL을 선택한 이유는, 상품 수나 필터 옵션, 할인 정보가 초 단위로 변하지 않으면서도 너무 오래된 데이터를 보여주지 않기 위한 균형점이었습니다.</p>\n<p><strong>캐싱 대상의 범위:</strong></p>\n<p>Redis에 캐싱하는 데이터는 <strong>변동이 적은 메타데이터</strong>에 한정했습니다. 검색 필터 옵션, 전체 상품 수, 즉시할인 데이터 등이 이에 해당합니다. 반면 <strong>가격, 재고 등 실시간 정합성이 중요한 데이터는 캐싱하지 않고</strong>, OpenSearch 검색 후 DB에서 직접 조회하여 반영합니다.</p>\n<h3 id=\"결과-핵심-검색-엔드포인트-p95-200ms-이하-달성\">결과: 핵심 검색 엔드포인트 p95 200ms 이하 달성</h3>\n<ul>\n<li><strong>검색 페이지 렌더링 (<code>/search</code>, p95)</strong>: 122.9ms</li>\n<li><strong>실제 상품 검색 API (<code>/api/items/search</code>, p95)</strong>: 194.9ms</li>\n<li>기존 1<del>2초 대비 엔드포인트별 **약 5</del>16배 개선**</li>\n</ul>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"결과-숫자로-증명하는-개선-효과\">결과: 숫자로 증명하는 개선 효과</h2>\n<h3 id=\"정량적-성과\">정량적 성과</h3>\n<table>\n<thead>\n<tr>\n<th>지표</th>\n<th>Before</th>\n<th>After</th>\n<th>개선율</th>\n<th>측정 기준</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>검색 페이지 렌더링 (<code>/search</code>, p95)</strong></td>\n<td>1~2초</td>\n<td>122.9ms</td>\n<td><strong>약 8~16배</strong></td>\n<td>에브리유니즈 액세스 로그 (1주 관측)</td>\n</tr>\n<tr>\n<td><strong>상품 검색 API (<code>/api/items/search</code>, p95)</strong></td>\n<td>1~2초</td>\n<td>194.9ms</td>\n<td><strong>약 5~10배</strong></td>\n<td>에브리유니즈 액세스 로그 (1주 관측)</td>\n</tr>\n<tr>\n<td><strong>CTR</strong></td>\n<td>17%</td>\n<td>22.5%</td>\n<td><strong>+5.5%p (상대 32% 향상)</strong></td>\n<td>세션 기반 A/B 테스트 (p &lt; 0.01)</td>\n</tr>\n<tr>\n<td><strong>상품 상세 페이지 진입률</strong></td>\n<td>기준군</td>\n<td>실험군</td>\n<td><strong>11.75% 증가</strong></td>\n<td>세션 기반 A/B 테스트 (p &lt; 0.01)</td>\n</tr>\n<tr>\n<td><strong>동의어 업데이트</strong></td>\n<td>30분</td>\n<td>5분</td>\n<td><strong>6배 단축</strong></td>\n<td>운영 요청 1건 처리 리드타임</td>\n</tr>\n</tbody></table>\n<h3 id=\"비즈니스-임팩트\">비즈니스 임팩트</h3>\n<ul>\n<li><strong>서버 안정성 확보</strong>: 서버 메모리에 상품 데이터를 적재하지 않아 서버 지표가 안정적으로 개선</li>\n<li><strong>운영 효율</strong>: 검색 관련 개발 요청 80% 감소 (마케팅/운영팀 셀프 서비스)</li>\n</ul>\n<h3 id=\"기술적-성과\">기술적 성과</h3>\n<ul>\n<li><strong>확장성</strong>: 상품 수 증가에도 검색 성능 유지 (OpenSearch의 분산 아키텍처)</li>\n<li><strong>안정성</strong>: 서버 메모리 부담 제거로 서버 전체 안정성 향상</li>\n<li><strong>유지보수성</strong>: 신규 랭킹 요소 추가 시 1일 작업 (기존 1주)</li>\n</ul>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"배운-점\">배운 점</h2>\n<h3 id=\"1-검색은-quot찾기quot가-아니라-quot예측quot이다\">1. 검색은 &quot;찾기&quot;가 아니라 &quot;예측&quot;이다</h3>\n<p>검색 엔진을 개편하면서 깨달은 점은, 검색이 단순히 데이터를 찾는 것이 아니라 <strong>사용자가 원하는 것을 예측하는 것</strong>이라는 점입니다. BM25 텍스트 유사도만으로는 사용자가 실제로 클릭하는 상품을 상위에 노출할 수 없었고, CTR 기반 랭킹을 결합해서야 검색 품질이 체감될 만큼 개선되었습니다.</p>\n<h3 id=\"2-캐싱은-quot어디에-적용할지quot가-핵심이다\">2. 캐싱은 &quot;어디에 적용할지&quot;가 핵심이다</h3>\n<p>OpenSearch는 강력하지만 모든 요청을 직접 처리하면 비용이 높습니다. 메타데이터와 즉시할인 데이터처럼 변동이 적은 데이터만 선별하여 캐싱하고, 가격·재고처럼 실시간 정합성이 중요한 데이터는 캐싱하지 않는 판단이 중요했습니다.</p>\n<h3 id=\"3-운영팀이-스스로-할-수-있는-구조를-만들어야-한다\">3. 운영팀이 스스로 할 수 있는 구조를 만들어야 한다</h3>\n<p>동의어·사용자 사전을 코드에 하드코딩하던 구조에서, 운영팀이 어드민에서 직접 수정하면 S3 → Lambda를 통해 자동 반영되는 구조로 전환했습니다. 검색 품질 개선 요소를 운영팀이 직접 관리할 수 있게 되면서, 개발팀은 검색 인프라 고도화에 집중할 수 있었습니다.</p>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"기술-스택\">기술 스택</h2>\n<ul>\n<li><strong>검색 엔진</strong>: AWS OpenSearch Service</li>\n<li><strong>백엔드</strong>: NestJS (TypeScript)</li>\n<li><strong>캐시</strong>: ElastiCache (Redis)</li>\n<li><strong>데이터 분석</strong>: Amazon Athena</li>\n<li><strong>자동화</strong>: AWS Lambda, S3, EventBridge</li>\n<li><strong>모니터링</strong>: CloudWatch</li>\n</ul>\n</section>\n"
  },
  {
    "id": "redis-writeback-optimization",
    "title": "배너 성과 집계 API 성능 최적화",
    "tags": [
      "Redis",
      "MySQL",
      "Lambda",
      "EventBridge",
      "Write-back"
    ],
    "content": "<section class=\"content-section\">\n<h2 id=\"목차\">목차</h2>\n<ol>\n<li><a href=\"#%EB%B0%B0%EA%B2%BD-%EC%9D%BC%EC%9D%BC-70%EB%A7%8C-%EA%B1%B4%EC%9D%98-%EB%B0%B0%EB%84%88-%EC%9D%B4%EB%B2%A4%ED%8A%B8-%EC%88%98%EC%A7%91\">배경: 일일 70만 건의 배너 이벤트 수집</a></li>\n<li><a href=\"#%EB%AC%B8%EC%A0%9C-%EB%B6%84%EC%84%9D-row-level-lock%EA%B3%BC-deadlock\">문제 분석: Row-level Lock과 Deadlock</a></li>\n<li><a href=\"#%ED%95%B4%EA%B2%B0-%EB%B0%A9%EC%95%88-%EA%B2%80%ED%86%A0-3%EA%B0%80%EC%A7%80-%EC%98%B5%EC%85%98-%EB%B9%84%EA%B5%90\">해결 방안 검토: 3가지 옵션 비교</a></li>\n<li><a href=\"#%EC%95%84%ED%82%A4%ED%85%8D%EC%B2%98-%EC%84%A4%EA%B3%84-redis-write-back-%EC%A0%84%EB%9E%B5\">아키텍처 설계: Redis Write-back 전략</a></li>\n<li><a href=\"#%ED%95%B5%EC%8B%AC-%EA%B5%AC%ED%98%84-1-redis-hash%EB%A1%9C-%EC%8B%A4%EC%8B%9C%EA%B0%84-%EC%A7%91%EA%B3%84\">핵심 구현 1: Redis Hash로 실시간 집계</a></li>\n<li><a href=\"#%ED%95%B5%EC%8B%AC-%EA%B5%AC%ED%98%84-2-eventbridge-lambda-%EB%B0%B0%EC%B9%98-%EB%8F%99%EA%B8%B0%ED%99%94\">핵심 구현 2: EventBridge + Lambda 배치 동기화</a></li>\n<li><a href=\"#%ED%95%B5%EC%8B%AC-%EA%B5%AC%ED%98%84-3-sqs-fallback%EC%9C%BC%EB%A1%9C-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%9C%A0%EC%8B%A4-%EB%B0%A9%EC%A7%80-%EC%B4%88%EA%B8%B0-%EC%9A%B4%EC%98%81-%EB%8B%A8%EA%B3%84\">핵심 구현 3: SQS Fallback으로 데이터 유실 방지</a></li>\n<li><a href=\"#%EA%B2%B0%EA%B3%BC-db-%EB%B6%80%ED%95%98-10%EB%B0%B0-%EA%B0%90%EC%86%8C\">결과: DB 부하 10배 감소</a></li>\n</ol>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"배경-일일-70만-건의-배너-이벤트-수집\">배경: 일일 70만 건의 배너 이벤트 수집</h2>\n<p>에브리타임 혜택탭이 출시되면서 거의 모든 페이지에 프로모션 배너가 배치되었습니다. 배너가 노출되거나 클릭될 때마다 성과 집계 API가 호출되어, <strong>전체 API 중 호출량 1위</strong>를 기록하게 되었습니다.</p>\n<p><strong>트래픽 규모:</strong></p>\n<pre><code class=\"language-\">- 일일 배너 이벤트(노출+클릭): 약 70만 건\n- 평균 초당 약 8건이지만, 인기 배너에 동시 요청이 집중되는 핫스팟 패턴</code></pre><p>평균 수치만 보면 MySQL이 충분히 처리할 수 있는 수준이지만, 문제는 트래픽이 균등하게 분산되지 않는다는 점입니다. 특정 프로모션 배너에 노출/클릭이 집중되면 <strong>같은 row에 동시 UPDATE가 몰려</strong> Lock 경합이 발생합니다. 이는 단순 트래픽 과부하가 아닌, <code>INSERT ON DUPLICATE KEY UPDATE</code>의 구조적 Lock 메커니즘에 기인한 문제였습니다.</p>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"문제-분석-row-level-lock과-deadlock\">문제 분석: Row-level Lock과 Deadlock</h2>\n<h3 id=\"기존-구조의-문제\">기존 구조의 문제</h3>\n<p><strong>배너 이벤트 발생 시 동작:</strong></p>\n<pre><code class=\"language-javascript\">router.post(&quot;/api/banner/performance/impression&quot;, async (req, res, next) =&gt; {\n  const { bannerId } = req.body;\n  const date = new Date();\n\n  await usMapper.queryWithValues(`\n    INSERT INTO banner_performance (banner_id, impression, click, date) VALUES (?)\n    ON DUPLICATE KEY UPDATE impression = impression + 1\n  `, [[bannerId, 1, 0, date]]);\n\n  apiHandler.send(res, { success: true }, next);\n});</code></pre><p><strong>문제점:</strong></p>\n<p><strong>1. <code>INSERT ON DUPLICATE KEY UPDATE</code>의 Lock 경합</strong></p>\n<p>이 구문은 단순 UPDATE와 달리, 먼저 Shared Lock(S)을 획득한 뒤 중복 키 감지 시 Exclusive Lock(X)으로 업그레이드하는 과정을 거칩니다. 같은 키(banner_id + date)에 동시 요청이 들어오면 Lock 대기가 발생합니다.</p>\n<pre><code class=\"language-\">요청 A: INSERT 시도 → S Lock 획득 → 중복 감지 → X Lock 대기\n요청 B: INSERT 시도 → S Lock 획득 → 중복 감지 → X Lock 대기\n요청 C: INSERT 시도 → S Lock 획득 → 중복 감지 → X Lock 대기</code></pre><p><strong>2. Deadlock 발생</strong></p>\n<p>동시에 같은 키에 여러 요청이 <code>INSERT ON DUPLICATE KEY UPDATE</code>를 실행하면, S Lock → X Lock 업그레이드 과정에서 서로의 S Lock 해제를 기다리며 교착 상태가 발생합니다.</p>\n<pre><code class=\"language-\">트랜잭션 A: S Lock 획득 → X Lock 필요 (B의 S Lock 대기)\n트랜잭션 B: S Lock 획득 → X Lock 필요 (A의 S Lock 대기)\n→ Deadlock!</code></pre><pre class=\"mermaid\">sequenceDiagram\n    participant A as 트랜잭션 A\n    participant DB as MySQL (banner_performance)\n    participant B as 트랜잭션 B\n\n    A->>DB: INSERT ON DUPLICATE KEY UPDATE (S Lock 획득)\n    B->>DB: INSERT ON DUPLICATE KEY UPDATE (S Lock 획득)\n    A->>DB: 중복 감지 → X Lock 필요\n    Note over A,DB: B의 S Lock 해제 대기\n    B->>DB: 중복 감지 → X Lock 필요\n    Note over DB,B: A의 S Lock 해제 대기\n    Note over A,B: DEADLOCK — 서로의 S Lock을 기다리며 교착</pre><p><strong>3. 다른 API에 대한 연쇄 영향</strong></p>\n<p>배너 집계 API의 DB 부하로 인해 같은 RDS를 사용하는 다른 API들도 느려지는 현상이 발생했습니다.</p>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"해결-방안-검토-3가지-옵션-비교\">해결 방안 검토: 3가지 옵션 비교</h2>\n<h3 id=\"방안-1-데이터-구조-변경-row-insert\">방안 1: 데이터 구조 변경 (Row INSERT)</h3>\n<p>집계 테이블 UPDATE 대신 이벤트를 Row로 저장하는 방식입니다.</p>\n<pre><code class=\"language-sql\">-- 클릭 이벤트를 Row로 저장\nINSERT INTO banner_clicks (banner_id, clicked_at) VALUES (1, NOW());\n\n-- 집계는 별도 쿼리\nSELECT COUNT(*) FROM banner_clicks WHERE banner_id = 1 AND clicked_at &gt;= '2024-05-01';</code></pre><p><strong>불채택 이유:</strong></p>\n<ul>\n<li>데이터 적재량 증가로 인한 스토리지 비용 부담 (일일 70만 건 × 365일)</li>\n<li>실시간 집계 쿼리(<code>COUNT</code>, <code>GROUP BY</code>)가 느림</li>\n<li>개발 리소스 및 인프라 변경 비용이 높음</li>\n</ul>\n<h3 id=\"방안-2-메시지-큐-비동기-처리\">방안 2: 메시지 큐 비동기 처리</h3>\n<p>SQS 등 메시지 큐로 이벤트를 비동기 처리하는 방식입니다.</p>\n<p><strong>불채택 이유:</strong></p>\n<ul>\n<li>운영 복잡도 증가</li>\n<li>메시지 큐 인프라 추가 비용</li>\n<li>결국 DB에 쓰는 구조는 동일하여 근본적 해결이 아님</li>\n</ul>\n<h3 id=\"방안-3-redis-집계-배치-동기화-채택\">방안 3: Redis 집계 + 배치 동기화 (채택)</h3>\n<p>Redis에서 실시간 집계 후 주기적으로 DB에 동기화하는 Write-back 방식입니다.</p>\n<p><strong>채택 이유:</strong></p>\n<ul>\n<li>기존 ElastiCache 인프라를 그대로 활용 (추가 비용 없음)</li>\n<li>성능, 비용, 안정성 모두 균형</li>\n<li>DB 부하를 근본적으로 해소</li>\n</ul>\n<blockquote>\n<p>당시에는 &quot;고빈도 카운터 성능 문제를 가장 빠르게 해소&quot;하는 것이 1순위였기 때문에 Redis Write-back이 최적의 선택이었습니다.<br>다만 이후 정산/과금 데이터로 활용 범위가 확대되면서, 운영 과정에서 예외 처리 비용과 정합성 관리 비용이 예상보다 크게 증가했고 아키텍처를 재검토하게 되었습니다.</p>\n</blockquote>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"아키텍처-설계-redis-write-back-전략\">아키텍처 설계: Redis Write-back 전략</h2>\n<h3 id=\"전체-아키텍처\">전체 아키텍처</h3>\n<pre class=\"mermaid\">graph TB\n    A[배너 이벤트] --> B[API Server]\n    B --> C[Redis HINCRBY]\n    C -->|성공| D[즉시 응답]\n\n    B -.Redis 실패.-> E[SQS Fallback]\n    E --> F[SQS Consumer]\n    F --> G[MySQL RDS]\n    J[배너 성과 조회 API] --> G\n\n    H[EventBridge Scheduler] -->|10분마다| I[AWS Lambda]\n    I -->|SCAN으로 데이터 조회| C\n    I -->|Bulk REPLACE| G</pre><h3 id=\"배치-주기-일일-1회에서-10분으로-변경\">배치 주기: 일일 1회에서 10분으로 변경</h3>\n<p>초기에는 일일 1회(오전 6시, 트래픽 최소 시간대) 배치로 운영했습니다. 그러나 Redis 장애 발생 시 하루치 데이터가 유실될 위험이 있어, <strong>10분 주기로 변경</strong>했습니다.</p>\n<table>\n<thead>\n<tr>\n<th>주기</th>\n<th>장점</th>\n<th>단점</th>\n<th>결정</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1분</td>\n<td>데이터 유실 최소화</td>\n<td>DB 부하 여전히 높음</td>\n<td>❌</td>\n</tr>\n<tr>\n<td><strong>10분</strong></td>\n<td><strong>유실 위험 최소화 + DB 부하 감소</strong></td>\n<td><strong>약간의 데이터 지연</strong></td>\n<td><strong>✅</strong></td>\n</tr>\n<tr>\n<td>1시간</td>\n<td>DB 부하 크게 감소</td>\n<td>유실 시 1시간 데이터 손실</td>\n<td>❌</td>\n</tr>\n<tr>\n<td>일일 1회</td>\n<td>DB 부하 최소</td>\n<td>장애 시 하루치 데이터 유실</td>\n<td>❌ (기존)</td>\n</tr>\n</tbody></table>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"핵심-구현-1-redis-hash로-실시간-집계\">핵심 구현 1: Redis Hash로 실시간 집계</h2>\n<h3 id=\"redis-hash-구조\">Redis Hash 구조</h3>\n<p>단순 카운터 누적 용도에는 인메모리 데이터 스토어가 적합합니다. 개별 String Key 대신 Hash Key로 통합하여 메모리를 최적화했습니다.</p>\n<p><strong>데이터 모델:</strong></p>\n<pre><code class=\"language-\">Key: {date}:{banner_id}\nFields:\n  - IMPRESSION: 노출 수\n  - CLICK: 클릭 수\n명령어: HINCRBY</code></pre><p><strong>예시:</strong></p>\n<pre><code class=\"language-\">20240501:1\n  IMPRESSION: 12345\n  CLICK: 890\n\n20240501:2\n  IMPRESSION: 8765\n  CLICK: 432</code></pre><p>Redis는 싱글스레드로 동작하기 때문에 HINCRBY 명령어로 Lock 경합 없이 안정적으로 값을 누적할 수 있습니다. MySQL에서 발생하던 Row-level Lock과 Deadlock 문제가 원천적으로 해소됩니다.</p>\n<p><strong>TTL 및 메모리 관리:</strong></p>\n<p>각 키에 7일 TTL을 설정했습니다. 배치 동기화가 정상 작동하면 DB에 이미 반영된 데이터이므로 7일 이상 보관할 필요가 없습니다.</p>\n<pre><code class=\"language-\">- 일당 활성 배너 수: 약 200~300개\n- 키 수: 일당 약 2,000개 (배너 × 날짜)\n- 7일 보관: 약 14,000개 키\n- 키당 크기: Hash 2필드 (IMPRESSION, CLICK) ≈ 100바이트\n- 총 메모리: 약 1.4MB</code></pre><p>기존 ElastiCache 인스턴스에서 유의미한 메모리 영향이 없는 수준이며, 별도의 메모리 관리 전략이 불필요했습니다.</p>\n<h3 id=\"api-구현\">API 구현</h3>\n<p><strong>이벤트 카운트 증가:</strong></p>\n<pre><code class=\"language-typescript\">@Post('/banners/:id/click')\nasync incrementClick(@Param('id') bannerId: number) {\n  const dateKey = dayjs().format('YYYYMMDD');\n\n  try {\n    // Redis HINCRBY (원자적 연산, Lock 없음)\n    await this.redis.hincrby(\n      `${dateKey}:${bannerId}`,\n      'CLICK',\n      1\n    );\n    return { success: true };\n\n  } catch (error) {\n    // Redis 실패 시 SQS Fallback\n    await this.sqsService.sendMessage({\n      type: 'BANNER_CLICK',\n      bannerId,\n      timestamp: Date.now()\n    });\n    return { success: true, fallback: true };\n  }\n}</code></pre><p><strong>조회 API (DB only):</strong></p>\n<p>배너 성과 조회 API는 <strong>Redis를 직접 조회하지 않고 DB만 단일 소스(Source of Truth)로 사용</strong>합니다.<br>Write 경로만 Redis를 거치고, Read 경로는 항상 <code>banner_daily_stats</code>를 조회하도록 분리했습니다.</p>\n<ul>\n<li>장점: 조회 로직 단순화, 이중 합산/정합성 오류 방지</li>\n<li>트레이드오프: 최대 10분 지연 허용</li>\n<li>운영 보강: 매일 00시 이후 전일 데이터 1회 최종 REPLACE로 마감 정합성 보장</li>\n</ul>\n<pre><code class=\"language-typescript\">@Get('/banners/:id/stats')\nasync getStats(\n  @Param('id') bannerId: number,\n  @Query('startDate') startDate: string,\n  @Query('endDate') endDate: string,\n) {\n  // 조회는 DB 단일 소스만 사용\n  const stats = await this.statsRepository.findByDateRange(\n    bannerId, startDate, endDate,\n  );\n\n  return {\n    impressions: stats.impressions,\n    clicks: stats.clicks,\n  };\n}</code></pre><hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"핵심-구현-2-eventbridge-lambda-배치-동기화\">핵심 구현 2: EventBridge + Lambda 배치 동기화</h2>\n<h3 id=\"배치-아키텍처\">배치 아키텍처</h3>\n<p>EventBridge Scheduler가 Lambda를 두 가지 모드로 트리거합니다.</p>\n<ol>\n<li><strong>정기 동기화 (10분마다)</strong>: 당일 데이터를 REPLACE</li>\n<li><strong>전일 최종 확정 (자정 이후 1회)</strong>: 전일 데이터(D-1)를 한 번 더 REPLACE</li>\n</ol>\n<p>전일 최종 확정 작업으로 날짜 경계(23:59~00:xx) 구간의 지연 이벤트까지 정리해, 조회 API가 DB만 바라봐도 일자별 정합성을 유지할 수 있습니다.</p>\n<p><strong>Lambda 함수:</strong></p>\n<pre><code class=\"language-typescript\">export const handler = async () =&gt; {\n  let cursor = '0';\n  const statsData = [];\n\n  // 1. SCAN으로 Redis 데이터 조회\n  do {\n    const [nextCursor, keys] = await redis.scan(\n      cursor, 'MATCH', '*:*', 'COUNT', 100\n    );\n    cursor = nextCursor;\n\n    for (const key of keys) {\n      const [date, bannerId] = key.split(':');\n      const stats = await redis.hgetall(key);\n\n      if (stats.IMPRESSION || stats.CLICK) {\n        statsData.push({\n          date,\n          bannerId: parseInt(bannerId),\n          impressions: parseInt(stats.IMPRESSION || '0'),\n          clicks: parseInt(stats.CLICK || '0'),\n        });\n      }\n    }\n  } while (cursor !== '0');\n\n  if (statsData.length === 0) return;\n\n  // 2. RDS에 Bulk REPLACE (누적값 통째로 교체)\n  // 정산 안전장치: GREATEST로 카운트 역행 방지 (아래 &quot;운영 보강&quot; 참조)\n  await db.transaction(async (trx) =&gt; {\n    await trx.raw(`\n      INSERT INTO banner_daily_stats (date, banner_id, impressions, clicks)\n      VALUES ${statsData.map(() =&gt; '(?, ?, ?, ?)').join(', ')}\n      ON DUPLICATE KEY UPDATE\n        impressions = GREATEST(banner_daily_stats.impressions, VALUES(impressions)),\n        clicks = GREATEST(banner_daily_stats.clicks, VALUES(clicks))\n    `, statsData.flatMap(s =&gt; [s.date, s.bannerId, s.impressions, s.clicks]));\n  });\n\n  // 3. TTL 7일 이상 된 Redis 데이터 정리\n  await cleanupOldRedisData(7);\n};</code></pre><p><strong>전일 최종 확정 스케줄 (예시):</strong></p>\n<pre><code class=\"language-yaml\"># 10분마다 정기 동기화\nBannerStatsSyncSchedule:\n  Type: AWS::Scheduler::Schedule\n  Properties:\n    ScheduleExpression: rate(10 minutes)\n    Target:\n      Input: '{&quot;mode&quot;:&quot;regular&quot;}'\n\n# 매일 00:05 전일(D-1) 최종 REPLACE\nBannerStatsFinalizeSchedule:\n  Type: AWS::Scheduler::Schedule\n  Properties:\n    ScheduleExpression: cron(5 0 * * ? *)\n    Target:\n      Input: '{&quot;mode&quot;:&quot;finalize_yesterday&quot;}'</code></pre><p><strong>REPLACE 방식의 데이터 정합성:</strong></p>\n<p>배치 동기화는 INCREMENT(증분)가 아닌 <strong>REPLACE(교체)</strong> 방식입니다. Redis에 저장된 값은 HINCRBY로 계속 누적되는 값이므로, DB에는 Redis의 최신 누적값으로 통째로 교체합니다.</p>\n<p>이 방식의 장점은 정합성 관리가 단순해진다는 점입니다:</p>\n<ul>\n<li><strong>SCAN~INSERT 사이에 새로운 HINCRBY가 발생하면?</strong> → 다음 배치에서 최신 누적값으로 교체되므로 유실 없음</li>\n<li><strong>Lambda가 DB INSERT 중 실패하면?</strong> → Redis 데이터는 그대로 유지되므로, 다음 배치에서 최신 값으로 재시도하여 자동 복구</li>\n<li><strong>같은 데이터가 2번 동기화되면?</strong> → 동일한 값으로 교체될 뿐, 중복 적산되지 않음</li>\n</ul>\n<p><strong>운영 보강 (정산 데이터 안전장치):</strong></p>\n<p>정산/과금 데이터는 카운트 역행(값 감소)을 허용할 수 없기 때문에, 단순 REPLACE 대신 아래 가드레일을 추가했습니다.</p>\n<ol>\n<li><strong>역행 방지 Upsert (<code>GREATEST</code>)</strong></li>\n</ol>\n<pre><code class=\"language-sql\">INSERT INTO banner_daily_stats (date, banner_id, impressions, clicks)\nVALUES (?, ?, ?, ?)\nON DUPLICATE KEY UPDATE\n  impressions = GREATEST(banner_daily_stats.impressions, VALUES(impressions)),\n  clicks = GREATEST(banner_daily_stats.clicks, VALUES(clicks));</code></pre><ol start=\"2\">\n<li><strong>역행 감지 알람</strong></li>\n</ol>\n<ul>\n<li>동기화 시 <code>incoming &lt; current</code>가 감지되면 해당 키 동기화를 중단</li>\n<li>CloudWatch + Slack 알람으로 즉시 운영자 개입</li>\n</ul>\n<p>이 가드레일로 Redis 데이터 이상 시 DB 카운트가 감소하는 사고를 방지했습니다.</p>\n<h3 id=\"scan을-사용하는-이유\">SCAN을 사용하는 이유</h3>\n<p><code>KEYS</code> 명령어는 전체 키를 한 번에 탐색하여 Redis를 블로킹합니다. 반면 <code>SCAN</code>은 반복적으로 점진 탐색을 수행하기 때문에, Redis의 싱글스레드 특성상 다른 요청을 차단하지 않고 부하를 최소화합니다.</p>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"핵심-구현-3-sqs-fallback으로-데이터-유실-방지-초기-운영-단계\">핵심 구현 3: SQS Fallback으로 데이터 유실 방지 (초기 운영 단계)</h2>\n<h3 id=\"redis-장애-시나리오\">Redis 장애 시나리오</h3>\n<ul>\n<li>Redis 클러스터 재시작</li>\n<li>네트워크 장애</li>\n<li>메모리 부족 (OOM)</li>\n</ul>\n<p>이런 상황에서도 배너 성과 데이터를 유실하면 안 됩니다.</p>\n<blockquote>\n<p>이 Fallback은 <strong>초기 운영 단계의 보호장치</strong>로 도입했습니다.<br>정산/과금 기준이 강화된 이후에는 <code>date + banner_id</code> 기준 REPLACE 동기화에 <code>GREATEST</code> 역행 방지와 역행 감지 알람을 주 정합성 전략으로 적용했습니다.</p>\n</blockquote>\n<h3 id=\"sqs-fallback-메커니즘\">SQS Fallback 메커니즘</h3>\n<p><strong>API에서 SQS 전송:</strong></p>\n<pre><code class=\"language-typescript\">async incrementClick(bannerId: number) {\n  try {\n    // 1차: Redis 시도\n    const dateKey = dayjs().format('YYYYMMDD');\n    await this.redis.hincrby(`${dateKey}:${bannerId}`, 'CLICK', 1);\n    return { success: true };\n\n  } catch (error) {\n    // 2차: SQS Fallback\n    this.logger.warn(`Redis failed, fallback to SQS: ${bannerId}`);\n\n    await this.sqs.sendMessage({\n      QueueUrl: process.env.BANNER_STATS_QUEUE_URL,\n      MessageBody: JSON.stringify({\n        type: 'BANNER_CLICK',\n        bannerId,\n        timestamp: Date.now()\n      })\n    }).promise();\n\n    return { success: true, fallback: true };\n  }\n}</code></pre><p><strong>SQS Consumer:</strong></p>\n<pre><code class=\"language-typescript\">@SqsMessageHandler('banner-stats-queue')\nasync handleMessage(message: Message) {\n  const payload = JSON.parse(message.Body);\n\n  // SQS 메시지는 DB에 직접 저장\n  await this.db('banner_daily_stats')\n    .where('banner_id', payload.bannerId)\n    .increment('clicks', 1);\n}</code></pre><p><strong>DLQ (Dead Letter Queue) 설정:</strong></p>\n<pre><code class=\"language-yaml\">BannerStatsQueue:\n  Type: AWS::SQS::Queue\n  Properties:\n    QueueName: banner-stats-queue\n    VisibilityTimeout: 60\n    RedrivePolicy:\n      deadLetterTargetArn: !GetAtt BannerStatsDLQ.Arn\n      maxReceiveCount: 3\n\nBannerStatsDLQ:\n  Type: AWS::SQS::Queue\n  Properties:\n    QueueName: banner-stats-dlq\n    MessageRetentionPeriod: 1209600  # 14일 보관</code></pre><hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"결과-db-부하-10배-감소\">결과: DB 부하 10배 감소</h2>\n<h3 id=\"지표-정의와-측정-기준\">지표 정의와 측정 기준</h3>\n<table>\n<thead>\n<tr>\n<th>지표</th>\n<th>값</th>\n<th>측정 기준/기간</th>\n<th>출처</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Write IOPS 감소율</strong></td>\n<td><strong>10배 이상 감소</strong></td>\n<td>적용 전/후 동일 트래픽 구간 비교 (운영 관측)</td>\n<td>RDS 모니터링 대시보드</td>\n</tr>\n<tr>\n<td><strong>RDS CPU 사용률</strong></td>\n<td><strong>약 20% 감소</strong></td>\n<td>적용 전/후 평균 CPU 비교</td>\n<td>CloudWatch <code>CPUUtilization</code></td>\n</tr>\n<tr>\n<td><strong>EC2 CPU 사용률</strong></td>\n<td><strong>약 8% 감소</strong></td>\n<td>적용 전/후 평균 CPU 비교</td>\n<td>CloudWatch <code>CPUUtilization</code></td>\n</tr>\n<tr>\n<td><strong>Row-level Lock/Deadlock</strong></td>\n<td><strong>운영 관측상 해소</strong></td>\n<td>적용 후 동일 이벤트 피크 구간 관찰</td>\n<td>DB lock/deadlock 로그</td>\n</tr>\n</tbody></table>\n<h3 id=\"성능-개선\">성능 개선</h3>\n<table>\n<thead>\n<tr>\n<th>지표</th>\n<th>개선 내용</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>Write IOPS</strong></td>\n<td>10배 이상 감소</td>\n</tr>\n<tr>\n<td><strong>RDS CPU 사용률</strong></td>\n<td>약 20% 감소</td>\n</tr>\n<tr>\n<td><strong>EC2 CPU 사용률</strong></td>\n<td>약 8% 감소</td>\n</tr>\n<tr>\n<td><strong>Row-level Lock</strong></td>\n<td>완전 해소</td>\n</tr>\n<tr>\n<td><strong>Deadlock</strong></td>\n<td>완전 해소</td>\n</tr>\n</tbody></table>\n<p>ElastiCache 측은 CPU나 메모리 사용률에 유의미한 영향이 없었습니다. 단순 카운터 누적은 Redis에게 매우 가벼운 연산입니다.</p>\n<h3 id=\"비용\">비용</h3>\n<table>\n<thead>\n<tr>\n<th>항목</th>\n<th>변경 내용</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>ElastiCache</strong></td>\n<td>기존 인프라 활용 (추가 비용 없음)</td>\n</tr>\n<tr>\n<td><strong>EventBridge + Lambda</strong></td>\n<td>월 $1 미만</td>\n</tr>\n<tr>\n<td><strong>SQS</strong></td>\n<td>Fallback 전용 (월 $0.5 미만)</td>\n</tr>\n</tbody></table>\n<p>비용 지표는 AWS 청구서 기준 월 평균 관측값입니다. 기존 인프라를 그대로 활용하면서 성능, 비용, 안정성 모두 개선할 수 있었습니다.</p>\n<h3 id=\"다른-api-성능-회복\">다른 API 성능 회복</h3>\n<p>DB 부하가 감소하면서 같은 RDS를 사용하는 다른 API들의 응답 속도도 함께 개선되었습니다(운영 관측). 다만 서비스별 절대 응답시간은 트래픽/쿼리 특성이 달라 공통 단일 수치로 제시하지 않았습니다.</p>\n<h3 id=\"운영-후-회고-아키텍처-전환-결정\">운영 후 회고: 아키텍처 전환 결정</h3>\n<p>Redis Write-back은 <strong>성능 문제를 빠르게 해결</strong>하는 데는 효과적이었습니다. 그러나 실제 운영에서는 설계 단계에서 과소평가했던 비용이 드러났습니다.</p>\n<ul>\n<li>날짜 경계(23:59~00:xx) 정합성 보정</li>\n<li>Redis 장애/재시작 시 역행 방지 로직</li>\n<li>fallback, 재처리, 알람 운영 복잡도 증가</li>\n<li>정산/과금 분쟁 대응을 위한 원본 근거 데이터 요구</li>\n</ul>\n<p>결론적으로, 정산 데이터의 Source of Truth를 Redis 집계값에 두는 방식은 장기적으로 불리하다고 판단했습니다. 이후에는 <strong>Firehose 기반 이벤트 원본 수집(S3 append-only) + 정산용 재집계 파이프라인</strong>으로 전환했습니다.</p>\n<p>전환 후 구조는 다음 원칙을 따릅니다.</p>\n<ul>\n<li>원본 이벤트는 append-only로 보관 (감사 추적 가능)</li>\n<li>정산 집계는 event_id 기준 dedup + watermark 기반 마감</li>\n<li>Redis는 정산 원장이 아니라 조회 성능 보조 용도로만 사용</li>\n</ul>\n<pre class=\"mermaid\">graph TB\n    A[배너 이벤트] -->|클릭/노출| B[API Server]\n    B -->|이벤트 전송| C[Firehose]\n    C -->|Parquet 변환| D[\"S3 (append-only 원본)\"]\n    D -->|event_id dedup| E[Athena / Redshift]\n    E -->|watermark 마감| F[정산 집계]\n\n    B -.->|조회 성능 보조| G[Redis]\n    G -.->|캐시 only| H[조회 API]\n    F -->|정산 원장| H</pre><hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"배운-점\">배운 점</h2>\n<p><strong>1. 단순 카운터에는 인메모리 스토어가 적합</strong></p>\n<ul>\n<li>MySQL의 UPDATE + Lock 구조는 고빈도 카운터에 부적합</li>\n<li>Redis HINCRBY는 싱글스레드 특성으로 Lock 없이 원자적 처리</li>\n</ul>\n<p><strong>2. 배치 주기는 데이터 유실 리스크와 타협</strong></p>\n<ul>\n<li>일일 1회: DB 부하 최소지만 장애 시 하루치 유실 위험</li>\n<li>10분: 유실 리스크와 DB 부하의 균형점</li>\n<li>EventBridge + Lambda로 서버리스 배치 구현</li>\n</ul>\n<p><strong>3. SCAN은 프로덕션의 필수</strong></p>\n<ul>\n<li>KEYS 명령어는 Redis를 블로킹하여 서비스 장애 유발 가능</li>\n<li>SCAN으로 점진 탐색하여 부하 최소화</li>\n</ul>\n<p><strong>4. Fallback 메커니즘은 필수</strong></p>\n<ul>\n<li>Redis 장애는 언제든 발생 가능</li>\n<li>SQS + DLQ로 데이터 유실 방지</li>\n<li>추가 비용은 거의 없음</li>\n</ul>\n<p><strong>5. Bulk REPLACE의 효과</strong></p>\n<ul>\n<li>개별 INSERT 대비 DB 트랜잭션 횟수를 대폭 감소</li>\n<li>10분간 누적된 데이터를 한 번의 트랜잭션으로 동기화</li>\n</ul>\n<hr>\n</section>\n<section class=\"content-section\">\n<h2 id=\"기술-스택\">기술 스택</h2>\n<table>\n<thead>\n<tr>\n<th>분류</th>\n<th>기술</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><strong>캐시/집계(초기)</strong></td>\n<td>Redis (ElastiCache)</td>\n</tr>\n<tr>\n<td><strong>데이터베이스</strong></td>\n<td>MySQL (RDS)</td>\n</tr>\n<tr>\n<td><strong>배치 스케줄러</strong></td>\n<td>AWS EventBridge Scheduler</td>\n</tr>\n<tr>\n<td><strong>배치 처리</strong></td>\n<td>AWS Lambda</td>\n</tr>\n<tr>\n<td><strong>Fallback</strong></td>\n<td>AWS SQS + DLQ</td>\n</tr>\n<tr>\n<td><strong>정산 파이프라인(후속 전환)</strong></td>\n<td>AWS Firehose, S3, Athena/Redshift</td>\n</tr>\n</tbody></table>\n</section>\n"
  }
]